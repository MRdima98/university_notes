\documentclass[12pt]{article}
\usepackage{listings}
\usepackage[hidelinks]{hyperref}
\newtheorem{theorem}{Theorem}
\usepackage{import}
\usepackage{mathpartir}
\usepackage{mathtools}
\usepackage{xcolor}
\usepackage{textcomp}
\usepackage[italian]{babel}
\usepackage{graphicx}
\usepackage{outlines}
\usepackage{tikz}
\usepackage{forest}
\usepackage{tikz-qtree}
\usetikzlibrary{automata, positioning}
\AddToHook{cmd/section/before}{\clearpage} 
\newcommand{\circumdelta}{%
  \leavevmode\vbox{
    \offinterlineskip
    \ialign{%
      \hfil##\hfil\cr
      \^{}\cr\noalign{\vskip-1ex}
      $\delta$\cr
    }
  }%
}
\lstnewenvironment{mycode}[1][]
{
  \lstset{
    language=Java, % Set the programming language
    basicstyle=\ttfamily, % Set the font for the code
    numbers=left, % Show line numbers
    numberstyle=\tiny, % Style of line numbers
    stepnumber=1, % Step between line numbers
    numbersep=5pt, % Space between line numbers and code
    tabsize=4, % Tab size
    breaklines=true, % Automatically break long lines
    breakatwhitespace=true, % Break lines at whitespace
    keywordstyle=\color{blue}, % Keywords in blue
    commentstyle=\color{gray}, % Comments in gray
    stringstyle=\color{red}, % Strings in red
    captionpos=b, % Caption at the bottom
    frame=single, % Add a frame around the code
    #1 % Any additional customizations provided when calling the environment
  }
}
{}


\begin{document}
\tableofcontents
\newpage

\section{Introduzione}
\subsection{Motivazione}
Un linguaggio è uno strumento per descrivere come risolvere i problemi in maniera rigorosa, in modo tale che sia eseguibile da un calcolatore
Perché è utile studiare come creare un linguaggio di programmazione?
\begin{itemize}
	\item non rimanere degli utilizzatori passivi
	\item capire il funzionamento dietro le quinte di un linguaggio
	\item domain-specific language (DSL): è un linguaggio pensato per uno specifico problema
	\item model drivern software development: modo complesso per dire UML e simili
	\item model checking
\end{itemize}


\subsection{Definizioni base}
Un linguaggio è composto da:
\begin{itemize}
	\item lessico e sintassi
	\item compilatore: parser + generatore di codice oggetto
\end{itemize}
La generazione automatica di codice può essere dichiarativa lessico
(espressioni regolari o automa a stati finite) o sintassi(grammatiche o automa a pile).
Un automa a stati finiti consuma informazioni una alla volta, ne salva una quantità finita. Alcuni esempi di applicazione di automa a stati finiti: software di progettazione di circuiti, analizzatore lessicale, ricerca di parole sul web e protocolli di comunicazione.

\begin{figure}[ht]
	\includegraphics[scale = 0.3]{media/semplice_automa.png}
	\centering
	\caption{Semplice automa}
\end{figure}

\subsection{Contenuti del corso}
\begin{outline}
	\1 Linguaggi formali e Automi:
	\2 Automi a stati finiti, espressioni regolari, grammatiche libere, automi a pila, Macchine di Turing, calcolabilità
	\1 Compilatori:
	\2 Analisi lessicale, analisi sintattica, analisi semantica, generazione di codice
	\1 Logica di base:
	\2 Logica delle proposizioni e dei predicati
	\1 Modelli computazionali:
	\2 Specifica di sistemi tramite sistemi di transizione, logiche temporali per la specifica e verifica di proprietà dei sistemi (model checking), sistemi concorrenti (algebre di processi e reti di Petri)
\end{outline}

\subsection{Informazioni utili}
Parte integrante del corso:
\begin{outline}
	\1 Supporto alla parte teorica usando tool specifici.
	\2 JFLAP 7.1: http://www.jflap.org (automi/grammatiche)
	\2 Tina 3.7.5: http://projects.laas.fr/tina
	(model checking di sistemi di transizione e reti di Petri)
	\2 LTSA 3.0: http://www.doc.ic.ac.uk/ltsa
	(sistemi di transizione definiti tramite algebre di processi)
	\1 Nel resto del corso utilizzeremo un ambiente di sviluppo per
	generare parser/compilatori
	\2 IntelliJ esteso con plug-in ANTLRv4, ultima versione 1.20
	(generatore ANTLR: http://www.antlr.org/)
\end{outline}

\newpage
Libri di testo suggeriti:
\begin{outline}
	\1 J. E. Hopcroft, R. Motwani e J. D. Ullman:
	Automi, linguaggi e calcolabilita’,
	Addison-Wesley, Terza Edizione, 2009. Cap. 1–9
	\1 A. V. Aho, M. S. Lam, R. Sethi e J. D. Ullman:
	Compilatori: principi tecniche e strumenti,
	Addison Wesley, Seconda Edizione, 2009. Cap. 1–5
	\1 M. Huth e M. Ryan:
	Logic in Computer Science: Modelling and Reasoning about
	Systems,
	Cambridge University Press, Second Edition, 2004. Cap. 1–3
\end{outline}

\section{Linguaggi regolari}
\subsection{Alfabeti}
Un \emph{alfabeto} è un insieme finito e non vuoto di simboli, comunemente indicato con $\Sigma$. Seguono alcuni esempi di alfabeti:
\begin{outline}
	\1 $\Sigma$ = \{0,1\} alfabeto binario
	\1 $\Sigma$ = \{a,b,...,z\} alfabeto di tutte lettere minuscole
	\1 L'insieme ASCII
\end{outline}

\subsubsection{Stringhe}
Una stringa/parola è un insieme di simboli di un alfabeto, 0010 è una stringa che appartiene $\Sigma$ = \{0,1\}.
\\ La \emph{stringa vuota} è una stringa composta da 0 simboli.
\\ La lunghezza della stringa sono il numeri di caratteri che la compongono (non devono essere unici). La sintassi per la lunghezza di una stringa w è $|w|$, quindi $|001|$ = 3 oppure $|\epsilon| = 0$ (nota bene, $\epsilon \ne 0$ ma è di lunghezza 0).

\subsubsection*{Potenze di un alfabeto}
Se $\Sigma$ è un alfabeto si può esprimere l'insieme di tutte le stringhe di una certa lunghezza con una notazione esponenziale: $\Sigma^k$ denota tutte le stringhe di lunghezza k con simboli che appartengono a $\Sigma$. \\
Per esempio:
\\ $\Sigma^1$ = \{0,1\}
\\ $\Sigma^2$ = \{00, 01, 10, 11\}
\\ $\Sigma^2$ = \{000, 001, 010, 011, 100, 101, 110, 111\}
\\ L'insieme delle stringhe meno quella vuota è segnato come $\Sigma^+$, mentre l'insieme che include la stringa vuota è $\Sigma^*$,

\subsubsection{Concatenazione di stringhe}
Siano x e y stringhe, dove i è la lunghezza di x e j è la lunghezza di y, la stringa xy è la stringa risultata dalla concatenazione delle stringhe xy di lunghezza i+j.

\subsection{Definizione di linguaggio}
Un insieme di stringhe a scelta L $\subseteq\Sigma^*$ si definisce linguaggio su $\Sigma$.
\\ Un modo formale per definire un alfabeto è il seguente \{w $|$ enunciato su w\}, che si traduce in "w tale che enunciato su w".
\\ $\{0^n 1^n | n \ge 1 \}$ si traduce in "l’insieme di 0 elevato alla n, 1 alla n tale che n è maggiore o uguale a 1"

\section{Automa a stati finiti deterministico}
Un automa a stati finiti deterministico consiste in:
\begin{enumerate}
	\item Un insieme di stati finiti Q
	\item Un insieme di simboli di input, $\Sigma$
	\item Una funzione di transizione, che prende in input uno stato e un simbolo e restituisce uno stato. Tale funzione è spesso indicato con $\delta$ ed è usata per rappresentare i archi nella rappresentazione grafica. Ovvero sia \emph{q} uno stato, \emph{a} un input allora $\delta$(q,a) è lo stato \emph{p} tale che esista un arco da q a p.
	\item Uno stato iniziale (naturalmente che appartiene a Q)
	\item Un insieme di stati accettati finali F. Questo è un sottoinsieme di Q.
\end{enumerate}
Un automa a stati finiti deterministico è spesso chiamato con l'acronimo DFA e viene può essere rappresentato nella seguente maniera concisa:
\[A = (Q, \Sigma, \delta, q\textsubscript{0}, F)\]
Dove A rappresenta il DFA.

\subsection{ Elaborazione di stringhe }
Per elaborare una stringa è si definisce lo stato iniziale, quello finale e una serie di regole di transizione per poterci arrivare.
Se dovessi decodificare la stringa 01 il DFA risulterebbe:
\[A = (Q=\{q1,q2,q3\}, \{0,1\}, \delta, q\textsubscript{0}, \{q1\})\]
I stati sono i sequenti:
\\ $\delta(q\textsubscript{0},1)=q\textsubscript{0}$: leggo come primo stato 1, nessun progresso fatto
\\ $\delta(q\textsubscript{0},0)=q\textsubscript{2}$: leggo come primo stato 0, posso andare avanti e cercare un 1
\\ $\delta(q\textsubscript{2},1)=q\textsubscript{1}$: leggo 1 dopo lo 0, ho trovato la stringa
\\ $\delta(q\textsubscript{2},0)=q\textsubscript{2}$: leggo 0 dopo lo 0, non ho fatto progresso
\\ Nota bene: questa è una notazione arbitraria del libro, q1 e q2 si possono invertire.

\subsubsection{ Notazioni semplici per DFA }
\subsubsection*{ Diagramma di transizione }
Dato un DFA $A = (Q, \Sigma, \delta, q\textsubscript{0}, F)$ un suo diagramma di transizione è composto da:
\begin{outline}
	\1 Ogni stato Q è un nodo
	\1 Ogni funzione $\delta$ è una freccia
	\1 La freccia Start che denota il primo input
	\1 Gli stati accettati F hanno un doppio cerchio
\end{outline}
\begin{figure}[ht]
	\includegraphics[scale = 0.3]{media/diagramma_stato.png}
	\centering
	\caption{Diagramma di transizione}
\end{figure}

\subsubsection*{ Tabelle di transizione }
Una tabella di transizione è costituita nelle riga dalle funzioni $\delta$ e nelle colonne dagli input. Ogni incrocio equivale a uno stato della funzione $\delta$ con un input generico \emph{a}.

\begin{table}[ht]
	\centering
	\begin{tabular}{c | c | c}
		                                  & 0                  & 1                  \\
		\hline
		$\rightarrow$  q\textsubscript{0} & q\textsubscript{2} & q\textsubscript{0} \\
		$*$q\textsubscript{1}             & q\textsubscript{1} & q\textsubscript{1} \\
		q\textsubscript{2}                & q\textsubscript{2} & q\textsubscript{1} \\
	\end{tabular}
	\caption{Esempio di tabella}
\end{table}

La freccia è lo start e l'asterisco è lo stato finale.

\subsubsection{ Estensione della funzione di transizione di stringhe }
Allo scopo di poter seguire una sequenza di input ci serve definire una funzione di transizione estesa. Se $\delta$ è una funzione di transizione, chiameremo \circumdelta\space la sua funzione estesa.
La funzione estesa prende in input \emph{q} e una stringa \emph{w} e ritorna uno stato \emph{p}.
\\ Ogni stato viene calcolato grazie allo stato esteso precedente: \[\circumdelta(\emph{q,w}) = \delta(\circumdelta(\emph{q,x}), \emph{a})\]
\subsubsection*{Esempio}
L = \{ \emph{w} $|$ \emph{w} ha un numero pari di 0 e di 1 \}
\\ Nota bene: 0 (numero di simboli) è pari quindi conta come stato accettato, ed è l'unico stato accettato.
\\ \hspace*{0.4cm} q\textsubscript{0}: 0 e 1 sono pari
\\ \hspace*{0.4cm} q\textsubscript{1}: 0 pari 1 dispari
\\ \hspace*{0.4cm} q\textsubscript{2}: 1 pari 0 dispari
\\ \hspace*{0.4cm} q\textsubscript{3}: 0 dispari 1 dispari
\[A = (\{q0,q1,q2,q3\}, \{0,1\}, \delta, q\textsubscript{0}, \{q\textsubscript{0}\})\]

\begin{figure}[ht]
	\includegraphics[scale = 0.5]{media/stringhe_pari.png}
	\centering
	\caption{Diagramma}
\end{figure}

\begin{table}[ht]
	\centering
	\begin{tabular}{c | c | c}
		                                   & 0                  & 1                  \\
		\hline
		$\rightarrow *$ q\textsubscript{0} & q\textsubscript{2} & q\textsubscript{1} \\
		q\textsubscript{1}                 & q\textsubscript{3} & q\textsubscript{0} \\
		q\textsubscript{2}                 & q\textsubscript{0} & q\textsubscript{3} \\
		q\textsubscript{3}                 & q\textsubscript{1} & q\textsubscript{2} \\
	\end{tabular}
	\caption{Esempio funzioni}
\end{table}

\newpage
Ora applichiamo le funzione di transizione estesa per verificare che 110101 abbia 0 e 1 pari:
\begin{itemize}
	\item  \circumdelta$(q\textsubscript{0}, \epsilon)$ = q\textsubscript{0}
	\item  \circumdelta$(q\textsubscript{0}, 1)$ = $\delta(\circumdelta(q\textsubscript{0}, \epsilon),1)$ = $\delta(q\textsubscript{0},1)$ =  q\textsubscript{1}
	\item  \circumdelta$(q\textsubscript{0}, 11)$ = $\delta(\circumdelta(q\textsubscript{0}, 1),1)$ = $\delta(q\textsubscript{1},1)$ =  q\textsubscript{0}
	\item  \circumdelta$(q\textsubscript{0}, 110)$ = $\delta(\circumdelta(q\textsubscript{1}, 11),0)$ = $\delta(q\textsubscript{0},1)$ =  q\textsubscript{2}
	\item  \circumdelta$(q\textsubscript{0}, 1101)$ = $\delta(\circumdelta(q\textsubscript{0}, 110),1)$ = $\delta(q\textsubscript{2},1)$ =  q\textsubscript{3}
	\item  \circumdelta$(q\textsubscript{0}, 11010)$ = $\delta(\circumdelta(q\textsubscript{0}, 1101),0)$ = $\delta(q\textsubscript{3},0)$ =  q\textsubscript{1}
	\item  \circumdelta$(q\textsubscript{0}, 110101)$ = $\delta(\circumdelta(q\textsubscript{0}, 11010),1)$ = $\delta(q\textsubscript{1},1)$ =  q\textsubscript{0}

\end{itemize}
A ogni simbolo aggiunto posso usare la funzione estesa precedente per calcolare il prossimo stato, in questo caso la sequenza ha un numero pari di 0 e 1.

\section{Automa a stati finiti non deterministici}
Un NFA (nondeterministic finite automaton) può trovarsi contemporaneamente in diversi stati. L'automa "scommette" sul input su certe proprietà dell'input.
\\ I NFA sono spesso più succinti e facili da definire rispetto ai DFA, un DFA può avere un numero di stati addirittura esponenziale rispetto a un NFA. Ogni NFA può essere convertito in un DFA.

\newpage
\subsection{Descrizione informale}
A differenza di un DFA, una funzione di stato in un NFA può restituire 0 o più stati. Immaginiamo di dover identificare se una stringa finisce con 01.
\\ Di seguito il diagramma di transizione sarà il seguente.
\begin{figure}[ht]
	\includegraphics[scale = 0.5]{media/01_end.png}
	\centering
	\caption{NFA che accetta stringa che finisce con 01}
\end{figure}
Come è possibile notare q\textsubscript{0} può può restituire due stati se riceve uno 0. Il NFA esegue molteplici stadi alla ricerca del pattern (simile a un processo che si moltiplica).

\begin{figure}[ht]
	\includegraphics[scale = 0.5]{media/NFA_es.png}
	\centering
	\caption{Gli stati del NFA}
\end{figure}
Ogni volta che il NFA accetta uno stato 0 crea due processi, un q\textsubscript{1} e q\textsubscript{0}
A ogni successivo input tutti i processi vanno avanti, nel nostro caso il q\textsubscript{1} "muore". Al secondo giro viene creato q\textsubscript{1} che muore alla quarta iterazione perché non è l'ultimo simbolo. Durante la quarta iterazione nasce q\textsubscript{1} che alla quinta ci porta uno stato accettato.

\newpage
\subsection{Definizione formale}
Formalmente un NFA si definisce come un DFA.
\[A = (Q, \Sigma, \delta, q\textsubscript{0}, F)\]
\begin{enumerate}
	\item Un insieme di stati finiti Q
	\item Un insieme di simboli di input, $\Sigma$
	\item Una funzione di transizione, che prende in input uno stato e un simbolo e restituisce \emph{\textbf{un insieme di stati}}. Questa è l'unica differenza rispetto al DFA, dove ci viene restituito un singolo stato.
	\item Uno stato iniziale (naturalmente che appartiene a Q)
	\item Un insieme di stati accettati finali F. Questo è un sottoinsieme di Q.
\end{enumerate}

\begin{table}[ht]
	\centering
	\begin{tabular}{c || c | c}
		                                 & 0                                          & 1                      \\
		\hline\hline
		$\rightarrow$ q\textsubscript{0} & \{q\textsubscript{0}, q\textsubscript{1}\} & \{q\textsubscript{0}\} \\
		q\textsubscript{1}               & $\emptyset$                                & \{q\textsubscript{2}\} \\
		$*$q\textsubscript{2}            & $\emptyset$                                & $\emptyset$            \\
	\end{tabular}
	\caption{Tabella di transizione di una NFA che accetta una stringa che finisce con 01}
\end{table}
L'unica differenza con una tabella DFA è che negli incroci ci sono dei insiemi di stati di output (singoletto quanto è uno solo), mentre se la transizione non esiste viene segnata con $\emptyset$.

\newpage
\subsection{Funzione di transizione estesa}
Come per i DFA bisogna prendere la funzione di transizione e renderla estesa. In questo caso lo stato precedente può ritorna un insieme di stati, quindi bisognare fare l'unione di questi. La funzione estesa di $\delta$ si chiamerà \circumdelta.
\[\bigcup^k_{x=2}\delta(p_i,a) = \{r_1,r_2,... ,r_m\} \]
Usiamo \circumdelta\space per calcolare se la stringa 00101 finisce con 01.

\begin{enumerate}
	\item $\circumdelta(q_0, \epsilon) = \{q_0\}$
	\item $\circumdelta(q_0, 0) = \delta(q_0,0) = \{q_0, q_1\}$
	\item $\circumdelta(q_0, 00) = \delta(q_0,0) \cup \delta(q_1,0)  = \{q_0, q_1\} \cup \emptyset = \{q_0, q_1\} $
	\item $\circumdelta(q_0, 001) = \delta(q_0,1) \cup \delta(q_1, 1) = \{q_0\} \cup \{q_2\} = \{q_0, q_2\}$
	\item $\circumdelta(q_0, 0010) = \delta(q_0,0) \cup \delta(q_2, 0) = \{q_0, q_1\} \cup \emptyset = \{q_0, q_1\}$
	\item $\circumdelta(q_0, 00101) = \delta(q_0,1) \cup \delta(q_1, 1) = \{q_0\} \cup \{q_2\} = \{q_0, q_2\}$
\end{enumerate}
Abbiamo un risultato positivo, $q_2$ mentre $q_0$ viene scartato

\subsection{Linguaggio NFA}
Come abbiamo visto sopra, il fatto di avere uno stato non accettabile al termine dell'operazione non significa che non abbia avuto successo.
\\ Formalmente se $A = (Q, \Sigma, \delta, q\textsubscript{0}, F)$ è un NFA allora:
\[L(A) = \{ w | \circumdelta(q_0, w) \cap \emph{F} \ne \emptyset \}\]
In parole povere L(A) è l'insieme delle stringhe w in $\Sigma^*$ tale che \circumdelta($q_0,w)$ contenga almeno uno stato accettante.

	\newpage
	\subsection{Equivalenza tra DFA e NFA}
	Di solito è più facile ottenere un NFA piuttosto che un DFA per un linguaggio. Nel migliori dei casi un DFA ha circa tanti stati quanti un NFA, ma più transizioni. Nel caso peggiore un DFA ha $2^n$ stati, mentre un NFA n.
	\\ Come detto in precedenza ogni NFA può essere ricondotto a un DFA, questo andrà dimostrato costruendo un DFA per insiemi a partire da un NFA.
	\\ Dato un NFA $A = (Q_N, \Sigma, \delta_N, q\textsubscript{0}, F_N)$ possiamo costruire un DFA \\ $A = (Q_D, \Sigma, \delta_D, \{q_0\}, F_D)$ tale che L(D)=L(N) (che i linguaggio sono uguali).
	\\ Si noti che i due linguaggi condividono lo stesso alfabeto.
	\\ Gli altri D componenti sono fatti nel seguente modo:
	\begin{outline}
		\1 $Q_D$ è formato da un insieme di insiemi di $Q_N$, in termini formali $Q_D$ è l'insieme potenza di $Q_N$. Quindi se $Q_N$ ha \emph{n} stati allora $Q_D$ ha $2^n$ stati, questo è vero nella teoria, nella pratica gli stati non raggiungibili non contano quindi tendono a essere meno di $2^n$.
		\1 $F_D$ è l'insieme dei sottoinsiemi di S di $Q_N$ tale che \emph{S}$ \cap F_N \ne \emptyset$. $F_D$ è quindi formato dagli sottoinsiemi di stati \emph{N} che includono almeno uno stato accettante.
		\1 Per ogni insieme \emph{S} $\subseteq Q_N$ e per ogni simbolo \emph{a} in $\Sigma$,
		\[\delta_D(S,a) = \bigcup_{p\;in\;S} \delta_N(p,a)\]
	\end{outline}
	Ovvero l'insieme $\delta_D(S,a)$ è calcolato tramite l'unione di tutti gli insiemi p in S.
	\begin{table}[ht]
		\centering
		\begin{tabular}{c || c | c}
			                        & 0                 & 1               \\
			\hline \hline
			$ \emptyset $           & $ \emptyset $     & $ \emptyset $   \\
			$ \rightarrow \{q_0\} $ & $ \{q_0, q_1\} $  & $ \{q_0\} $     \\
			$ \{q_1\} $             & $ \emptyset $     & $ \{q_2\} $     \\
			$ *\{q_2\} $            & $ \emptyset $     & $ \emptyset $   \\
			$ \{q_0,q_1\} $         & $  \{q_0, q_1\} $ & $ \{q_0,q_2\} $ \\
			$ *\{q_0,q_2\} $        & $ \{q_0, q_1\} $  & $ \{q_0\} $     \\
			$ *\{q_1,q_2\} $        & $ \emptyset $     & $ \{q_2\} $     \\
			$ *\{q_0, q_1, q_2\} $  & $ \{q_0,q_1\} $   & $ \{q_0,q_2\} $ \\
		\end{tabular}
		\caption{Stringa che termina con 01, NFA $\rightarrow$ DFA}
	\end{table}

	La tabella precedente era deterministica nonostante fosse formata da insiemi, \emph{ogni insieme è uno stato}, e non sono insieme di stati. Per rendere più chiara l'idea possiamo cambiare notazione.
	\begin{table}[ht]
		\centering
		\begin{tabular}{c || c | c}
			                             & 0 & 1 \\
			\hline \hline
			\hphantom{*$\rightarrow$}A   & A & A \\
			\hphantom{*}$\rightarrow$B   & E & B \\
			\hphantom{*$\rightarrow$}C   & A & D \\
			\hphantom{$\rightarrow$}$*$D & A & A \\
			\hphantom{*$\rightarrow$}E   & E & F \\
			\hphantom{$\rightarrow$}$*$F & E & B \\
			\hphantom{$\rightarrow$}$*$G & A & D \\
			\hphantom{$\rightarrow$}$*$H & E & F \\
		\end{tabular}
		\caption{Stringa che termina con 01, notazione nuova}
	\end{table}
	\\ Tra gli 8 stati presenti in tabella possiamo raggiungere: B, E e F. Gli atri stati sono irraggiungibili o non esistenti. È possibile evitare di costruire questi stati compiendo una "valuta differita".
	\\ Trattando i l'insieme di stati come un unico stato composto da un insieme è possibile riscrivere la DFA in questo modo:

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/nfa_to_dfa.png}
		\centering
		\caption{Grafico DFA convertito da NFA}
	\end{figure}

	\newpage
	\subsection*{}
	\subsubsection*{Teorema}
	Se $D = (Q_N, \Sigma, \delta_N, q\textsubscript{0}, F_N)$ è il DFA trovato per costruzione a partire dal NFA $N = (Q_D, \Sigma, \delta_D, \{q_0\}, F_D)$ allora L(D)=L(N).

	\subsubsection*{Teorema}
	Un linguaggio L è accettato da un DFA se e solo se L è accettato da un NFA.

	\section{Automa con epsilon-transazioni}
	Un estensione degli automa è la capacità di poter ammettere come input la stringa vuota $\epsilon$. È come se l'NFA compisse una transizioni spontaneamente. Tale NFA si chiamerà $\epsilon$-NFA

	\subsection{Uso delle epsilon-transizioni}
	L'esempio di seguito tratta le $\epsilon$ come invisibili, possono mutare lo stato ma non sono contante nella catena.

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/epsilon_nfa.png}
		\centering
		\caption{epsilon-NFA che accetta numeri decimali}
	\end{figure}

	L'$\epsilon$-NFA in figura accetta numeri decimali formati da:
	\begin{enumerate}
		\item un segno +,- facoltativo
		\item una sequenza di cifre
		\item un punto decimale
		\item una seconda sequenza di cifre
	\end{enumerate}
	È possibile avere input vuoti prima della virgola $\delta(q_1, .) = q_2$ e dopo la virgola $\delta(q_4, .) = q_3$ ma non entrambi. Il segno è facoltativo $\delta(q_0, \epsilon) = q_1$.
	\\ In $q_3$ l'automa può "scommettere" che la sequenza sia finita oppure può andare avanti a leggere.

	\newpage
	\subsection{Notazione formale di epsilon-NFA}
	La definizione forma di un $\epsilon$-NFA è uguale a quella di un NFA, va solo specificate le informazioni relative alla transizione $\epsilon$.
	\\ Una $\epsilon$-NFA è definita con $A = (Q, \Sigma, \delta, q_0, F)$, dove $\delta$ è una funzione di transizione che richiede come input:
	\begin{enumerate}
		\item uno stato \emph{Q}
		\item un elemento $\Sigma \cup \{\epsilon\}$, ovvero un simbolo di input oppure il simbolo $\epsilon$. Questa distinzione viene fatta per evitare confusione.
	\end{enumerate}

$\epsilon$-NFA per riconoscere un numero decimale
	\[ E = (\{q_0,q_1,...,q_5\}, \{.,+,-,1,...,9\},\delta, q_0, \{q_5\})\]

	\begin{table}[ht]
		\centering
		\begin{tabular}{c || c | c | c | c}
			      & $\epsilon$  & +,-         & .           & 0,1,...,9      \\
			\hline \hline
			$q_0$ & $\{q_1\}$   & $\{q_1\}$   & $\emptyset$ & $\emptyset$    \\
			$q_1$ & $\emptyset$ & $\emptyset$ & $\{q_2\}$   & $\{q_1, q_4\}$ \\
			$q_2$ & $\emptyset$ & $\emptyset$ & $\emptyset$ & $\{q_3\}$      \\
			$q_3$ & $\{q_5\}$   & $\emptyset$ & $\emptyset$ & $\{q_3\}$      \\
			$q_4$ & $\emptyset$ & $\emptyset$ & $\{q_3\}$   & $\emptyset$    \\
			$q_5$ & $\emptyset$ & $\emptyset$ & $\emptyset$ & $\emptyset$    \\
		\end{tabular}
		\caption{Tabella di transizione per un numero decimale}
	\end{table}

	\subsection{Epsilon chiusure}
	Un $\epsilon$-chiusura è un cammino fatto solo di transizioni $\epsilon$. Formalmente tale stato si scrive ENCLOSE(q) = {insieme di stati.}

	\subsection{Transizioni estese di epsilon-NFA}
	Grazie alle $\epsilon$-chiusure possiamo definire cosa significa accettare un input.
	\\
	Supponiamo $E = (Q, \Sigma, \delta, q_0, F)$ un $\sigma$-NFA, \circumdelta(q,w) è la funzione di transizione estesa le cui etichette concatenate descrivono la stringa w.
	\\
	\textbf{BASE} \circumdelta(q,w) = ENCLOSE(q), se l'etichetta è $\epsilon$ posso seguire solo cammini $\epsilon$, definizione di ENCLOSE.
	\\
	\textbf{INDUZIONE} Supponiamo \emph{w} abbia forma \emph{xa}, dove \emph{a} è l'ultimo simbolo, che non può essere $\epsilon$ perché non appartiene a $\Sigma$:
	\begin{enumerate}
		\item Poniamo \circumdelta(q,x) = $\{p_1, p_2,...,p_k\}$ in questo modo tutti i cammini $p_i$ sono tutti gli stati raggiungibili da q a x. Questi stati possono terminare con $\epsilon$ oppure contenere altre $\epsilon$ transizioni
		\item Sia $\bigcup^k_{i=1}\delta(p_i,a)$ l'insieme $\{r_1, r_2, ...,r_m\}$, ovvero tutte le transizioni da \emph{a} a \emph{x}.
		\item Infine \circumdelta(q,w) = $\bigcup^m_{j=1}ENCLOSE(r_j)$, questo chiude gli archi rimasti dopo \emph{a}
	\end{enumerate}
	Forma contratta
	\[\circumdelta(q,xa) = \bigcup_{p\in\circumdelta(q,x)}(\bigcup_{t\in\delta(q,a)}ENCLOSE(t))\]
	Il linguaggio accettato è L(E) = $\{w|\circumdelta(q_0,w) \cap F \ne 0\}$

	\subsection{Da epsilon-NFA a DFA}
	Dato un $\epsilon$-NFA possiamo costruire un equivalente DFA per sottoinsiemi. Sia $E = (Q_E, \Sigma, \delta_E, q_0, F_E)$ un $\epsilon$-NFA il suo equivalente DFA è
	\[ D = (Q_D, \Sigma, \delta_D, q_0, F_D) \]
	ovvero:
	\begin{enumerate}
		\item $Q_D$ è l'insieme di sottoinsiemi $Q_E$. Ogni stato accessibile in D è un sottoinsieme $\epsilon$-chiuso di $Q_E$, in termini formali S$\subseteq Q_E$ tale che S = ENCLOSE(S).
		\item $q_D$=ENCLOSE$(q_0)$
		\item $F_D$ contiene almeno uno stato accettante in E.
		      \\ $F_D=\{S|S$ è in $Q_D$ e $S \cap F_E\neq0\}$

		\item $\circumdelta(q,xa) = \bigcup_{p\in\circumdelta(q,x)}(\bigcup_{t\in\delta(q,a)}ENCLOSE(t))$
	\end{enumerate}
	\textbf{Teorema} Un linguaggio è linguaggio L è accetto da un $\epsilon$-NFA se è solo se è accettato da un DFA.

	\section{Espressioni regolari}
	Le espressioni regolari definiscono gli stessi linguaggi definiti dai vari automi: \emph{linguaggi regolari}. A differenza degli automi, le espressioni regolari descrivono linguaggi in maniera dichiarativa. Per questo motivo le espressioni regolari sono molto diffuse, per esempio nel commando unix \emph{grep} oppure negli analizzatori lessicali.

	\subsection{Operatori lessicali}
	L'espressione lessicale 01*+10* denota il linguaggio 0 seguito da qualsiasi numero di 1 oppure 1 seguito da qualsiasi numero di 0.
	\\ Per poter definire le operazioni sulle regex (sinonimo di espressione regolare) dobbiamo definire tali operazioni sui linguaggi che esse rappresentano:
	\begin{enumerate}
		\item \emph{Unione} di due linguaggi L ed M, L$\cup$M, indica tutte le stringhe che appartengono ad L e ad M oppure a entrambi.
		\item \emph{Concatenazione} di due linguaggi L ed M è l'insieme di stringhe formate dalla concatenazione di una qualsiasi stringa L con una qualsiasi stringa M. Tale operazione è indicata così: L$\cdot M$ oppure semplicemente LM.
		      Per L=\{001,10,111\} e M=\{$\epsilon$,001\}
		      LM=\{001,10,111,001001, 10001, 111001\}
		\item \emph{Chiusura}(o \emph{star} o chiusura di Kleene) di un linguaggio L, indicata come L*, rappresenta l'insieme delle stringhe che si possono formare tramite concatenazione e ripetizione di qualsiasi stringa in L. Nel caso L=\{0,1\} L* rappresenta l'alfabeto binario, qualsiasi combo di 0 e 1. Nel caso L=\{0, 11\} L* rappresenta qualsiasi stringa che abbia una o più coppie di 1, NB 011 è valido ma come 01111, mentre 101 non è valido, non abbiamo né la stringa 10 né la stringa 01. Formalmente L* è l'unione infinita $\bigcup_{i>0}L_i$ dove $L^0=\{\epsilon\}$, $L^1=L$, $L^i=LL...L$.
	\end{enumerate}

	\newpage
	\subsection{Proprietà regex}
	\begin{outline}
		\1 $L\cup M=M\cup L$ L'unione è commutativa
		\1 $(L\cup M) \cup M= L\cup (M\cup L)$ L'unione è associativa
		\1 $(LM) M= L (M L)$ La concatenazione è associativa (LM $\neq$ ML)
		\1 $\emptyset \cup L= L \cup \emptyset=L$
		\1 $\{\epsilon\} \cup L= L \cup \{\epsilon\}=L$
		\1 $\emptyset L= L \emptyset=\emptyset$
		\1 $L(M\cup N)= LM \cup LN$
		\1 $(M\cup N)L=ML\cup NL$
		\1 $L\cup L=L$
		\1 $\emptyset^*=\{\epsilon\}, \{\epsilon\}^* =\{\epsilon\}$
		\1 $L^+=LL^*=L^*L,\;L^*=L^*\cup \{\epsilon\}$
	\end{outline}

	\newpage
	\subsection{Costruzione di regex}
	Servono modi per raggruppare le espressioni regolari, in questo caso vengono usati operatori algebrici comuni. Di seguito verranno definite regex lecite E con il loro corrispondente linguaggio L(E).
	\\ \textbf{BASE}
	\begin{enumerate}
		\item le costanti $\epsilon$ e $\emptyset$ sono regex, rispettivamente del linguaggio \{$\epsilon$\} e \{$\emptyset$\}, in altri termini L($\epsilon$) =\{ $\epsilon$\} e L($\emptyset$) = \{$\emptyset$\}.
		\item Se a è un simbolo allora \textbf{a} è una regex che denota il linguaggio \{a\}, ovvero L(\textbf{a}) = \{a\}. (si usa il grassetto per distinguere simboli da regex)
		\item Una lettera maiuscola qualsiasi, di solito \emph{L}, viene usata per indicare un linguaggio arbitrario
	\end{enumerate}
	\textbf{INDUZIONE}
	\begin{enumerate}
		\item Data \emph{E} ed \emph{L} regex, allora \emph{E} + \emph{L} è una regex che indica l'unione dei due linguaggi L(\emph{E}) e L(\emph{L}), in altre parole L(\emph{E+F}) = L(\emph{E}) $\cup$ L(\emph{F})
		\item Date \emph{E} e \emph{F} due regex, \emph{EF} indica la concatenazione tra i due linguaggio L(\emph{E}) e L(\emph{F}), in altri termini L(\emph{EF}) = L(E)L(F).
		\item Data \emph{E} una regex, \emph{E}* indica la chiusura del linguaggio L(\emph{E}), in altri termini L(\emph{E}*) = (L(\emph{E}))*
		\item Data \emph{E} una regex, allora anche (\emph{E}) è una regex valida che appartiene sempre al linguaggio \emph{E}, in termini formali L((\emph{E})) = L(\emph{E})
	\end{enumerate}
	\textbf{Esempio di regex}
	\\ Si crei una regex che descriva una linguaggio che è fatto di 0 e 1 alternati.
	\\ Intuitivamente si potrebbe provare \textbf{01}*, che è errato, questo indica tutte le stringhe che hanno uno 0 e un numero arbitrario di 1. (\textbf{01})* è corretto, però indica per forza un linguaggio di 01 alternati, quindi 101010 non sarebbe valido
	\\ Uniamo regex per descrivere il caso: (\textbf{10}*) 10 alternato,
	\textbf{0(10)*} 10 con 0 all'inizio,
	\textbf{1(01)*} 01 con 1 all'inizio,
	in conclusione
	\[\textbf{(01)*+(10)*+0(10)*+1(01)*}\]
	Un modo più contratto sarebbe quello di aggiungere un 1 facoltativo all'inizio e uno 0 facoltativo alla fine
	\[\textbf{($\epsilon$+1)(01)*)($\epsilon$+0)}\]

	\subsection{Precedenza degli operatori}
	\begin{enumerate}
		\item Star ha la precedenza massima
		\item concatenazione
		\item unione
	\end{enumerate}
	Naturalmente si possono usare parentesi per decidere il proprio ordine e inoltre è consigliato farlo anche se non fosse necessario per rendere più chiara l'espressione.

	\section{Automa a stati finite e regex}
	Abbiamo visto che le regex e gli automi a stati finiti possono descrive gli stessi linguaggi, va solo dimostrato che formalmente.
	\\ Dobbiamo dimostrare che:
	\begin{enumerate}
		\item Ogni linguaggio definito da un automa è definito anche da una regex, useremo un DFA per comodità
		\item Ogni linguaggio definito da una regex è definita da un automa, useremo un $\epsilon$-NFA per comodità
	\end{enumerate}

	\begin{figure}[ht]
		\includegraphics[scale = 0.7]{media/regex_conv.png}
		\centering
		\caption{Conversioni}
	\end{figure}

	\subsection{Da DFA a regex}
	\textbf{Teorema}
	\\ Se L = L(A) per un DFA A, allora esiste una regex \emph{R} tale che L = L(\emph{R}).
	\\ Il procedimento formale e matematico è formato dal espansione di ogni singolo stato tramite la formula:
	\[ R^{k}_{ij} =  R^{k-1}_{ij} + R^{k-1}_{ik} (R^{k-1}_{kk} R^{k-1}_{kj})\]
	In parole povere sto calcolando l'espressione regolare da uno stato j a uno stato i k volte, una per ogni stato.
	\\ Questo procedimento è molto lungo, perché l'espressione va effettuata per ogni transizione, unita e poi ridotta.
	\\ Tenendo però a mente questa formalità è possibile usare un metodo più gestibile, ovvero \emph{l'eliminazione per stati}.

	\newpage

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/regex_dfa.png}
		\centering
		\caption{Eliminazione per stati}
	\end{figure}

	\begin{outline}
		\1 \emph{s} è lo stato generico che sta per essere eliminato
		\1 $q_1, q_2,...,q_k$ sono i k stati precedenti a \emph{s}
		\1 $Q_i$ sono tutte le transizioni precedenti
		\1 $p_1, p_2,...,p_k$ sono i k stati successi a \emph{s}
		\1 $P_i$ sono tutte le transizioni successive
		\1 $R_{ij}$ sono tutte le transizioni tramite regex, bisogna definirne una per ogni direzione \emph{ij} ma se non dovesse esistere basterà scrivere $\emptyset$
	\end{outline}

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/removed_s.png}
		\centering
		\caption{Eliminazione di s}
	\end{figure}

	A questo punto possiamo iniziare a costruire l'espressione regolare a partire dall'automa.
	\newpage
	\begin{enumerate}
		\item Bisogna eliminare tutti gli stati intermedi ad eccezione di $q_0$.
		\item Se $q_0\neq q_1$ allora questo stato può essere espresso come $E_q$ = (R + SU*T)*SU*, un cammino generico illustrato in figura.

		      \begin{figure}[ht]
			      \includegraphics[scale = 0.5]{media/automa_generico.png}
			      \centering
			      \caption{Automa generico 2 stati}
		      \end{figure}

		\item Se $q_0$ è accettante allora la regex è data da R*

		      \begin{figure}[ht]
			      \includegraphics[scale = 0.5]{media/automa_q0.png}
			      \centering
			      \caption{Automa generico 1 stato}
		      \end{figure}

	\end{enumerate}

	\newpage
	Dato un NFA come segue.

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/dfa_s1.png}
		\centering
		\caption{NFA esempio}
	\end{figure}

	Esprimiamo le sue funzioni di transizioni come regex

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/dfa_s2.png}
		\centering
		\caption{NFA con archi regex}
	\end{figure}

	Il primo stato che rimuoviamo è B, applicando la fromula. $R_{11} + Q_1 S^* P_1$, in questo caso risulta $\emptyset + 1\emptyset$*(0+1), che si può  ridurre in 1(0+1).
	\\ NB $\emptyset$* equivale a $\epsilon$, non annulla le regex, mentre $\emptyset$ sì


	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/dfa_s3.png}
		\centering
		\caption{B rimosso}
		\label{b rimosso}
	\end{figure}

	\newpage
	Eliminiamo C

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/dfa_s4.png}
		\centering
		\caption{C rimosso}
	\end{figure}

	Ora possiamo applicare (R + SU*T)*SU*, quindi
	\begin{itemize}
		\item R=(0+1)
		\item S=1(0+1)(0+1)
		\item T=$\emptyset$
		\item U=$\emptyset$.
	\end{itemize}
	\[((0+1) + 1(0+1)(0+1)\emptyset^*\emptyset)^*1(0+1)(0+1)\emptyset\]
	Possiamo semplificare U* perché è equivalente a $\epsilon$ e possiamo eliminare SU*T perché è T è $\emptyset$.
	\[(0+1)^*1(0+1)(0+1)\]
	Questo è lo stato accettante D, è necessario calcolare lo stato accettante C. Ripartendo dalla fig.\ref{b rimosso} applichiamo di nuovo $E_Q$ ottenendo (0+1)*1(0+1).
	\\ L'espressione finale è data dalla \textbf{somma} delle 2 espressioni.
	\[(0+1)*1(0+1) + (0+1)^*1(0+1)(0+1)\]

	\newpage
	\subsection{Da regex a automi}
	\textbf{Teorema} Per ogni rex R possiamo costruire un $\epsilon$-NFA A tale che L(R)=L(A).
	\\ Questo si dimostra per induzione strutturale, prendendo come base gli automi $\epsilon$, $\emptyset$ e \emph{a}.

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/epsilon.png}
		\includegraphics[scale = 0.5]{media/empty.png}
		\includegraphics[scale = 0.5]{media/simple_a.png}
		\centering
		\caption{Stati base}
	\end{figure}


	\begin{figure}[ht]
		\includegraphics[scale = 0.4]{media/RS.png} \\ \\
		\includegraphics[scale = 0.4]{media/RSmult.png} \\ \\
		\includegraphics[scale = 0.4]{media/RSstar.png}
		\centering
		\caption{R+S, RS e R*}
	\end{figure}

	R+S significa che viene percorso 1 dei 2 espressioni. RS significa che una volta percorso R, quello diventa lo stato iniziale di S. R* va in loop su se stesso.

	\newpage
	Usando i blocchi precedenti convertiamo (0+1)*1(0+1)

	\begin{figure}[ht]
		\includegraphics[scale = 0.4]{media/01.png} \\ \\
		\includegraphics[scale = 0.4]{media/01star.png} \\ \\
		\includegraphics[scale = 0.4]{media/01star01.png}
		\centering
		\caption{R+S, RS e R*}
	\end{figure}

	\section{Proprietà dei linguaggi regolari}
	\begin{outline}
		\1 \emph{Pumping Lemma}: Ogni linguaggio regolare soddisfa il pumping di lemma
		\1 \emph{Proprietà di chiusura}: Possibilità di costruire un nuovo automa a partire da altri automi, seguendo specifiche operazioni
		\1 \emph{Proprietà di decisione}: Analisi di automi, come l'equivalenza
		\1 \emph{Tecniche di minimizzazione}: Possiamo ridurre un automa
	\end{outline}

	\subsection{Pumping Lemma}
	Un linguaggio non è detto che sia regolare.
	\\ Immaginiamo di avere un linguaggio $L_{01}=\{0^n\;1^n|n\geq 1\}$. Questo è un linguaggio che accetta una stringa con tanti 1 quanti 0. Perché questo linguaggio possa essere un DFA deve avere un numero finito di stati, diciamo $k$. Quindi dopo $k+1$ simboli, $\epsilon,0,00,...,0^k$ ci troviamo in un qualche stato. Poiché gli stati sono limitati esistono 2 strade diverse per cui ci troviamo nello stesso stato, chiamiamoli $0^j$ e $0^i$.
	\\ Ora immaginiamo dallo stato $j$ di iniziare a leggere 1, l'automa deve fermarsi quando ha letto $j$ quantità di 1, ma non può farlo perché non ricorda lo stato, potrebbe finire dopo $i$ quantità di 1, $L_{01}$ non è regolare.
	\\ \textbf{Teorema} Sia L un linguaggio regolare, allora esiste una costante $n$ tale che, per ogni stringa $w$ in $L$ dove $|w|\geq n$ possiamo scomporre $w$ in 3 stringhe $w=xyz$ tale che:
	\begin{enumerate}
		\item $y \neq \epsilon$
		\item $|x,y| \leq n$
		\item per ogni $k \geq 0$ anche $xy^kz$ è in $L$
	\end{enumerate}
	Ovvero c'è una stringa non vuota replicabile da qualche parte, senza uscire dal linguaggio.
	\\ \textbf{Dimostrazione} Supponiamo che $L$ sia regolare. Allora L=L(A) e supponiamo che $A$ abbia $n$ stati. Ora consideriamo una stringa $w$ dove $w=a_1,a_2,...,a_m \; m \geq n$ e ogni $a_i$ è un simbolo di input. Definiamo la sua funzione $\delta(a_1,a_2,...,a_n)$ che descrivere tutte le $p_i$ transizioni, e $q_0=p_0$.
	\\ Per il principio della piccionata tutti gli stati non possono essere distinti, quindi esistono due stati $p_i$ e $p_j$ dove $0 \leq i \leq j \leq n$ tale che $p_i = p_j$. Possiamo scomporre w in w=xyz:

	\begin{enumerate}
		\item $x=a_1,a_2,...,a_i$
		\item $y=a_{i+1},a_{i+2},...,a_j$
		\item $x=a_{j+1},a_{j+2},...,a_m$
	\end{enumerate}

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/pump.png}
		\centering
		\caption{A un certo punto i stati si ripetono}
	\end{figure}

	Se k=0 siamo allo stato accettante, se $k \geq 0$ allora dobbiamo necessariamente fare dei loop, perché l'input è $xy^kz$.

	\newpage
	\subsection{Chiusura dei linguaggi regolari}
	Sia $L$ e $M$ due linguaggi regolari allora i seguenti sono a loro volta linguaggi regolari.
	\begin{outline}
		\1 \emph{Unione}: $L \cup M$
		\1 \emph{Intersezione}: $L \cap M$
		\1 \emph{Complemento}: $N$
		\1 \emph{Differenza}: $L \backslash M$
		\1 \emph{Inversione}: $LR = \{wR : w \in L\}$
		\1 \emph{Chiusura}: $L^*$
		\1 \emph{Concatenazione}: $L\cdot M$
	\end{outline}

	\textbf{Teorema} Sia L e M linguaggi regolari allora anche L $\cup$ M è un linguaggio regolare.
	\\ \textbf{Dimostrazione} L ed M sono linguaggi descritti dalle espressioni regolari S ed R, quindi L=L(S) e M=L(R) quindi L$\cup$M=L(R+S).
  \vspace{5mm}

\textbf{Teorema} Se L è un linguaggio regolare sull'alfabeto $\Sigma$ allora anche $\overline{L}=\Sigma^*-L$.
\\ \textbf{Dimostrazione} Sia L=L(A) per un DFA A=$(Q,\Sigma,\delta,q_0,F)$, allora $\overline{L}$=L(B) dove B è il DFA $(Q,\Sigma,\delta,q_0,Q-F)$, quindi B ha gli stati accentanti opposti a quelli di A. In questo caso l'unico modo per cui $w$ è in L(B) se e solo se $\delta(q_0,w)$ è in $Q-F$, ovvero \textbf{non} è in L(A).

\begin{figure}[ht]
  \includegraphics[scale = 0.5]{media/prop1.png}
  \centering
  \caption{Diagramma di A}
\end{figure}

\newpage
Il diagramma di B risulta opposto
\begin{figure}[ht]
  \includegraphics[scale = 0.5]{media/prop2.png}
  \centering
  \caption{Diagramma di B}
\end{figure}

\textbf{Teorema} 4.8. Se $L$ e $M$ sono regolari, allora anche $L \cap M$ è regolare.
\vspace{5mm}
\\ \textbf{Dimostrazione} Le 3 operazioni booleane sono interdipendenti, quindi possiamo usare le due precedenti per dimostrare l'Intersezione
\[L\cap M = \overline{\overline{L} \cup \overline{M}}\]
\\ \textbf{Dimostrazione alternativa} Sia L il linguaggio $L = (Q_L, \Sigma, \delta_L, q\textsubscript{0}, F_L)$ e M il linguaggio
$M = (Q_M, \Sigma, \delta_M, q\textsubscript{0}, F_M)$ assumiamo per semplicità che siano dei DFA
Se $A_L$ passa da $p$ a $s$ e $A_M$ passa da $q$ a $t$ (sono tutti stati) allora $A_{L\cap M}$ passera da $(p,s) a (q,t)$ quando legge una stringa $a$.
\\ Formalmente il linguaggio risultante dall'intersezione diventa 
\[A = (Q_M \times Q_L, \Sigma, \delta_{M\cap L}, (q_M, q_L), F_L \times F_M)\]
Si può dimostrare che $\circumdelta((q_M,q_L),w) = (\circumdelta(q_M, w), \circumdelta(q_L,w))$, questo perché A accetta solo quando entrambi gli stati sono accettanti, quindi accetta per forza anche l'intersezione. 

\subsubsection{Chiusura rispetto alla differenza} 
\textbf{Teorema} Sia $L$ e $M$ dei linguaggi regolari allora anche $L-M$ è un linguaggio regolare
\vspace{5mm}
\\ \textbf{Dimostrazione} $L-M=L\cap \overline{M}$, ma sappiamo che $\overline{M}$ è regolare e l'intersezione di 2 linguaggi è regolae, quindi $L-M$ è anche esso regolare.

\subsubsection{Inversione}
L'inversione di una strina $a_1, a_2, ...,a_n$ è la stringa $a_n,..., a_2, a_1$ questa stringa la denotiamo come $w^R$ e notiamo che $\epsilon^R=\epsilon$.
\\ Un linguaggio $L^R$ inverso di $L$ presenta tutte le stringe al suo interno inverse. Se $L$ è un linguaggio regolare allora lo è anche $L^R$.

\subsection{Proprietà di decisione}
Questi non scontanti che vanno affrontate:
\begin{itemize}
  \item Un linguaggio descritto è vuoto?
  \item Una stringa appartiene al linguaggio?
  \item Due linguaggi equivalenti?
\end{itemize}

\subsubsection{Verificare se un linguaggio è vuoto}
Se il linguaggio $A$ è rappresentato da un automa finito posso attraversare tutti i nodi, se trovo uno stato accettante allora il linguaggio \textbf{non} è vuoto. Quest'operazione richiede O($n^2$) perché è un semplice attraversamento di grafo. 
\\ Se iniziamo da un espressione regolare, possiamo trasformarla in un $\epsilon$-NFA e poi effettuare i cammini a costo O(n).
\\ È possibile anche determinare se il linguaggio è vuoto in base alla regex direttamente. Se il linguaggio non ha $\emptyset$ sicuramente non può essere vuoto, altrimenti posso determinarlo ricorsivamente seguendo le regole algebriche delle regex. 
\\ Di seguito elenco i casi: 
\begin{itemize}
  \item $R= \emptyset$. $L(\emptyset)$ è vuoto
  \item $R= \epsilon$. $L(\epsilon)$ \textbf{non} è vuoto
  \item $R=a$ (qualsiasi stringa $a$) L(a) non è vuoto
  \item $R=R_1+R_2$. $L(R)$ è vuoto se sia $L(R_1)$ che $L(R_2)$ siano vuoti
  \item $R=R_1R_2$. $L(R)$ è vuoto se $L(R_1)$ o $L(R_2)$ è vuoto
  \item $R=R_1*$. $L(R)$ non è mai vuoto, al massimo è $\epsilon$
  \item $R=R_1$. $L(R)$ è vuoto solo se $L(R_1)$ è vuoto, sono lo stesso linguaggio
\end{itemize}

\subsubsection{Appartenenza a un linguaggio}
Per controllare se una qualsiasi stringa $w\in L(A)$ per un DFA è sufficiente simulare w su A, se $|w|=n$ il tempo risulta O(n).
\vspace{5mm} \\
Se $A$ è un NFA e ha $s$ stati, allora O($ns^2$), vale lo stesso epr $\epsilon$-NFA.
\vspace{5mm} \\
Se L=L(R) è una regex, la converto in $\epsilon$-NFA ovvero O($ns^2$)

\subsubsection{Equivalenza e minimizzazione di automi}
Dobbiamo esplorare la possibilità di dire che 2 DFA sono equivalenti, un modo per farlo è minimizzarli, se sono equivalenti basterà cambiare etichette finché non coincidono.
\\ Iniziamo definendo cosa rende equivalenti 2 stati $p$ e $q$. Data una stringa $w$, $p$ e $q$ sono equivalenti se \circumdelta$(q,w)$ e \circumdelta$(p,w)$ sono entrambi accentanti oppure non accentanti. 
\\ Nel caso in cui uno sia accentante e l'altro no, allora si dicono distinti. NB. 2 stati equivalenti non ci dicono niente sulla stringa $w$ e non ci dice se i due stati sono lo stesso. 
\\ Possiamo raggruppare le nostre distinzioni degli stati in una tabella, tramite l'algoritmo $riempit-tabella$.

\begin{figure}[ht]
	\includegraphics[scale = 0.3]{media/riemp_tab.png}
	\centering
	\caption{Algoritmo riempi tabella}
\end{figure}
Ogni x è uno stato distinguibile, un quadrato vuoto significo stati equivalenti.
\\ Per testare se 2 linguaggi L e M sono equivalenti dobbiamo: 
\begin{itemize}
  \item Convertire L e M in DFA 
  \item Costruire il DFA unione dei 2 linguaggi
  \item Se l'algoritmo dice che i 2 stati iniziali sono equivalenti allora L=M, altrimenti L$\ne$M
\end{itemize}

Partendo da 2 DFA costruisco un DFA $B$ che ha lo stato iniziale che contiene quello di $A$ e lo stato accentante che contiene quello di $A$. Ogni altra funzione di un blocco deve essere equivalente.
\\ L'algoritmo non può essere applicato a un NFA.

\section{Grammatiche libere da contesto}
\textbf{Esempio informale}
\vspace{5mm}
\\ Definiamo un linguaggio delle palindrome. Una stringa è palindroma se si legge allo stesso modo in entrambi i versi, come $otto$ oppure $madamimadam$ (madame I'm Adam). Perciò $w$ è palindroma se $w=w^R$. 
\\ Si può facilmente dimostrare che questo linguaggio non è regolare usando il pumping lemma. Scegliamo $w=0^n10^n$, scomponiamo in $w=xyz$ tale che y sia fatto di vari 0 e scegliamo k=0, $xz$ dovrebbe adesso appartenere a $L_{pal}$ ma non è così, perché ho meno 0 a sinistra rispetto che a destra. 
\\ Posso definire le stringhe che appartengono a $L_{pal}$ in maniera ricorsiva. 
\vspace{2mm}
\\ \textbf{Base} $\epsilon$, 0 e 1 sono palindrome
\\ \textbf{Induzione} Se $w$ è palindroma allora $0w0$ e $1w1$ sono palindrome
\vspace{2mm}
\\ Una \textbf{grammatica libera} è una notazione formale per esprimere linguaggi ricorsivamente. Una grammatica consiste in una o più serie di variabili che rappresentano classi di stringhe, ovvero linguaggi.
\\ Ogni classe definisce come costruire le stringhe in ogni classe. Definiamo le classi del linguaggio palindroma:
\begin{enumerate}
  \item $P\rightarrow\epsilon$
  \item $P\rightarrow0$
  \item $P\rightarrow1$
  \item $P\rightarrow 0P0$
  \item $P\rightarrow 1P1$
\end{enumerate}
0 e 1 sono terminali, P è una variabile, P è anche la categoria iniziale, 1-5 sono produzioni.

\newpage
\subsection{Definzione formale di CFG}
Un CFG è formato da 4 elementi: 
\begin{enumerate}
  \item Un insieme di simboli detti \emph{terminali}
  \item Un insieme di variabili, detti \emph{non termali} oppure \emph{categorie sintattiche}
  \item Una variabile detto \emph{simbolo iniziale}
  \item Un insieme finito di \emph{produzioni} o \emph{regole} che definiscono il linguaggio ricorsivamente. Ogni produzione consiste in 3 parti 
    \begin{enumerate}
      \item Una variabile che è definita parzialmente dalla produzione, \emph{testa}
      \item Il simbolo di produzione $\rightarrow$
      \item Il \emph{corpo} della produzione, ovvero la stringa o il terminale che la forma. Le stringhe vengono formate sostituendo le variabili.
    \end{enumerate}
\end{enumerate}
In maniera contratta, CFG=(V,T,P,S) rispettivamente variabile, terminale, produzioni e simbolo iniziale. 
\\ Il linguaggio palindromo descritto come CFG è $G_{pal}=(\{P\}, \{0,1\},A,P)$ A è l'insieme delle produzioni del linguaggio.

\subsection{Derivazione in CFG}
Possiamo definire le stringhe tramite concatenazione delle produzione, \emph{inferenza ricorsiva} 
\\ Il secondo modo per \emph{derivazione}, ovvero uso le produzioni fino a quanto ho solo simboli terminali. Noi studieremo la derivazione.
\\ Sia $G=(V,T,P,S)$ una grammatica libera e sia $\alpha AB$ una stringa mista di terminali e variabili, dove $A$ è variabile e $\alpha B \in (V \cup T)$ allora se $G$ risulta chiara nel contesto, posso scrivere: 
\[\alpha A B \xRightarrow[G] \; \alpha \gamma B\]
$A$ è una derivazione di $G$, $A \Rightarrow \gamma $. Posso usare il simbolo * per denotare "zero o più passi"(chiusura transitiva). 
\\ \textbf{Base} $\alpha \xRightarrow[G]{*} \alpha$, vale per ogni stringa terminale o variabile, ovvero ogni stringa deriva se stessa 
\\ \textbf{Induzione} Se $\alpha \xRightarrow[G]{*} \beta$ e $\beta \xRightarrow[G]{} \gamma$ significa che $\alpha \xRightarrow[G]{*} \gamma$
\\ Se la grammatica è chiara posso scrivere $\xRightarrow{*}$

\subsection{Derivazione a sinistra e a destra}
È possibile arrivare a diverse conclusioni a seconda della scelta di quali produzioni fare, quindi per evitare di avere incertezze si può scegliere un verso di come derivare ogni volta.
\\ Derivazione a destra: $\xRightarrow[rm]{}$
\\ Derivazione a sinistra: $\xRightarrow[lm]{}$

\subsection{Linguaggio di una grammatica}
Se $G(V,T,P,S)$ è una CFG allora il suo linguaggio è:
\[L(G)=\{ w \in T^*:S \xRightarrow[G]{*} w \}\]
Ovvero l'insieme delle stringhe $T^*$ derivabili da $w$

\subsection{Alberi sintattici}
Data una grammatica $G=(V,T,S,P)$, gli alberi sintattici di $G$ soddisfano i seguenti requisiti: 
\begin{enumerate}
  \item Ogni nodo interno è una variabile $V$
  \item Ogni foglia è un variabili, terminale o $\epsilon$. Quando è $\epsilon$ deve essere l'unico figlio
  \item Se un nodo $A$ con i figli $X_1,X_2,...,X_k$ allora $A \rightarrow X_1,X_2,...,X_k$ è una produzione in $P$. $X$ può essere $\epsilon$ solo nel caso in qui $A \rightarrow \epsilon$
\end{enumerate}

Nella grammatica seguente: 
\begin{enumerate}
  \item $E \rightarrow I$
  \item $E \rightarrow E+E$
  \item $E \rightarrow E*E$
  \item $E \rightarrow (E)$
\end{enumerate}

\begin{figure}[ht]
  \includegraphics[scale = 0.5]{media/produzione.png}
  \centering
  \caption{Parsing tree}
\end{figure}
\newpage
Questo albero sintattico rappresenta la produzione 
$E \xRightarrow[]{*} I + E$

\subsubsection{Prodotto di un albero sintattico} 
Se concateniamo le foglie di un albero otteniamo una stringa, ovvero il \emph{prodotto}. Inoltre la radice deve essere il simbolo iniziale e ogni foglia è $\emptyset$ oppure un terminale. 

\begin{figure}[ht]
	\includegraphics[scale = 0.3]{media/prod_concat.png}
	\centering
	\caption{Produzione}
  \label{prod}
\end{figure}
\newpage
Il risultato della produzione è $a*(a+b00)$

\subsubsection{Inferenza, derivazione e alberi sintattici}
Sia $G=(V,T,P,S)$ una CFG e $A \in V$ allora i seguenti sono equivalenti: 
\begin{enumerate}
  \item $A \xRightarrow[]{*} w$ 
  \item $A \xRightarrow[lm]{*} w$ 
  \item $A \xRightarrow[rm]{*} w$ 
  \item C'è un albero sintattico $G$ con radice $A$ e prodotto $w$
\end{enumerate}
Costruiamo il prodotto della fig. \ref{prod} per derivazione sinistra: 
\begin{itemize}
  \item $ E \xRightarrow[lm]{} E * E \xRightarrow[lm]{}$
  \item $ I * E \xRightarrow[lm]{} $
  \item $ a * E \xRightarrow[lm]{} $
  \item $ a * (E) \xRightarrow[lm]{} $
  \item $ a * (E+E) \xRightarrow[lm]{} $
  \item $ a * (I+E) \xRightarrow[lm]{} $
  \item $ a * (a+E) \xRightarrow[lm]{} $
  \item $ a * (a+I) \xRightarrow[lm]{} $
  \item $ a * (a+I0) \xRightarrow[lm]{} $
  \item $ a * (a+I00) \xRightarrow[lm]{} $
  \item $ a * (a+b00) \xRightarrow[lm]{} $
\end{itemize}

\subsection{Ambiguità in grammatiche e linguaggi}
Nella grammatica: 
\begin{itemize}
  \item $E \rightarrow I$ 
  \item $E \rightarrow E+E$ 
  \item $E \rightarrow E*E$ 
  \item $E \rightarrow (E)$ 
\end{itemize}
L'espressione $E+E*E$ ha due possibili derivazioni: 
$E \Rightarrow E + E \Rightarrow E + E * E$ oppure 
$E \Rightarrow E * E \Rightarrow E + E * E$, da qui i due alberi sintattici

\begin{figure}[ht]
  \includegraphics[scale = 0.5]{media/ambigua.png}
  \centering
  \caption{Semplice automa}
\end{figure}

\newpage 
Queste sono 2 operazioni diverse, usiamo i valori $1+2*3$, dal primo albero troviamo $1+(2*3)=7$ mentre dal secondo albero troviamo $(1+2)*3=9$.
\\ \textbf{Definizione} Sia $G=(V,T,P,S)$ una CFG. Diciamo che G è ambigua se esiste almeno una stringa T* che ha più di un albero sintattico. 

\subsubsection{Rimuovere ambiguità}
È possibile, in alcuni casi, rimuovere l'ambiguità. Non esiste però un modo sistematico e alcuni CFL hanno solo CFG ambigue. 
\\ Studiamo la grammatica 
\[ E \rightarrow I \;|\; E + E \;|\; E * E \;|\; (E)\]
\[ I \rightarrow a \;|\; b \;|\; Ia \;|\; Ib \;|\; I0 \; | \; I1\]
Dobbiamo decidere: 
\begin{enumerate}
  \item Chi ha precedenza tra + e *
  \item Come raggruppare un operatore
\end{enumerate}
Una soluzione è l'introduzione di una gerarchia di variabili: 
\begin{enumerate}
  \item espressioni E: composizione di uno o più termini T tramite +
  \item termini T: composizione di uno o più fattori F tramite *
  \item fattori F:
    \begin{enumerate}
      \item identificatori I
      \item espressioni E racchiuse tra parentesi
    \end{enumerate}
\end{enumerate}
Ogni più è costretto a essere una T, ogni T può generare E solo chiuse nelle parentesi, questo significa che * ha precedenza rispetto al +. 
\\ La seguente grammatica risulta quindi non ambigua: 
\begin{enumerate}
  \item $E \rightarrow T | E + T$
  \item $E \rightarrow F | T * F$
  \item $F \rightarrow I | (E) $
  \item $E \rightarrow a | b | Ia | Ib | I0 | I1$
\end{enumerate}

\subsubsection{Ambiguità inerente} 
Un CFL è inerentemente ambiguo se tutte le grammatiche per L sono ambigue. 
\[ L = \{a^n b^n c^m d^m :n \geq 1, m \geq 1\} \cup \{a^n b^m c^m d^n : n \geq 1, m \geq 1\} \]
Il linguaggio è inerentemente ambiguo.

\section{Automi a pila}
Un automa a pila (PDA - pushdown automaton) è un $\epsilon$-NFA con una pila che è la sua memoria.
\\ Durante le transizione:
\begin{enumerate}
  \item Consuma un simbolo di input o esegue una transizione $\epsilon$
  \item Va in un nuovo stato o rimane dov'è
  \item Rimpiazza la cima della pila con una stringa (lo stack è cambiato), con $\epsilon$ (c'è stato un pop) oppure con lo stesso simbolo (nessun cambiamento)
\end{enumerate}

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/pda.png}
	\centering
	\caption{PDA, automa con pila}
\end{figure}
Esempio: Consideriamo il linguaggio palindromo: 
\[ L_{wwr} = \{ ww^R : w \in \{0,1\}^*\}\]
Una PDA per $L_{wwr}$ ha 3 stati: 
\begin{enumerate}
  \item In $q_0$ si presume l'input non sia esaurito, quindi leggo 1 alla volta tutti i simboli di input e li accumulo nello stack.
  \item Scommettiamo di aver trovato la sequenza corretta, quindi passiamo in $q_1$ ma continuando a leggere input
  \item Confronta la cima della pila con il simbolo in $q_1$, se sono uguali consumiamo l'elemento, altrimenti il ramo muore
  \item Se lo stack è vuoto passiamo in $q_2$
\end{enumerate}

\subsection{Definizione formale PDA}
Un PDA è una tupla di 7:
\[ P = (Q,\Sigma, \Gamma, \delta, q_0, Z_0,F)\]
dove 
\begin{itemize}
  \item Q è un insiemi di stati finiti
  \item $\Sigma$ è un alfabeto finito di input
  \item $\Gamma$ è un alfabeto finito di pila
  \item ($\delta$ è una funzione di transizione da $Q\times(\Sigma \cup \{\epsilon\}) \times \Gamma$ a sottoinsieme di $Q \times \Gamma^*$ - definizione del prof, poco chiara)
    \\ $\delta$ è un funzione di transizione che prende 3 input $(q,a, X)$ dove: 
    \begin{itemize}
      \item q è uno stato in Q
      \item a è un simbolo in $\Sigma$ oppure la $\epsilon$, NB $\epsilon$ non fa parte dell'input
      \item X è un simbolo in $\Gamma$, ovvero è un simbolo \textbf{sullo stack}
    \end{itemize}
    L'output di $\sigma$ è una coppia $(p, \gamma)$, dove p è uno stato e $\gamma$ è una nuova stringa che rimpiazza $X$ sullo stack. 
    Se $\gamma = \epsilon$ $X$ viene eliminato, se $\gamma = X$ non cambia nulla e se $\gamma = YZ$ allora $Y$ sostituisce $X$ e $Z$ viene aggiunto allo stack

  \item $q_0$ stato iniziale 
  \item $Z_0 \in \Gamma$ simbolo iniziale per la pila
  \item $F \subseteq Q$ è l'insieme degli stati di accettazione
\end{itemize}

\newpage
Prendiamo come esempio il PDA di $L_{wwr}$: 
\[P=(\{q_0, q_1, q_2\}, \{0,1\}, \{Z_0, 0, 1\}, \delta, q_0, Z_0, \{ q_2 \}\]

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/pda_diag.png}
	\centering
\end{figure}

dove $\delta$ è definita dalle seguenti regole: 
\begin{enumerate}
  \item $\delta(q_0,0,Z_0)$ = $\{(q_0, 0Z_0)\}$ 
        $\delta(q_0,1,Z_0)$ = $\{(q_0, 1Z_0)\}$ 
        una di queste regole è applicata all'inizio, l'input viene inserito lasciando $Z_0$ come indice del fondo.
  \item 
        $\delta(q_0,0,0)$ = $\{(q_0, 00)\}$,
        $\delta(q_0,0,1)$ = $\{(q_0, 01)\}$,
        \\ $\delta(q_0,1,0)$ = $\{(q_0, 10)\}$,
        $\delta(q_0,1,1)$ = $\{(q_0, 11)\}$
        \\ leggo un input, lo salvo nello stack e continuo a leggere 
  \item 
        $\delta(q_0,\epsilon,Z_0)$ = $\{(q_1, Z_0)\}$,
        \\ $\delta(q_0,\epsilon,0)$ = $\{(q_1, 0)\}$ 
        \\ $\delta(q_0,\epsilon,1)$ = $\{(q_1, 1)\}$ 
        \\ passiamo da $q_0$ a $q_1$ lasciando intantto lo stack
  \item 
        $\delta(q_1,0,0)$ = $\{(q_1, \epsilon)\}$,
        \\ $\delta(q_1,1,1)$ = $\{(q_1, \epsilon)\}$
        \\ rimaniamo su $q_1$ ma eliminiamo un membro dallo stack, se sono uguali
  \item 
        $\delta(q_1,\epsilon,Z_0)$ = $\{(q_2, Z_0)\}$
        se non ho niente nello stack, passo alla soluzione accettante

\end{enumerate}

\newpage
\subsection{Descrizioni istantanee}
Un PDA passa da una configurazione all'altra consumando un simbolo in input oppure la cima della stack.
\\ Possiamo rappresentare una configurazione tramite \emph{descrizioni istantanee} (ID) che sono un tupla $(q, w,\gamma)$ dove: q è lo stato, w è l'input rimanente e $\gamma$ è il contenuto della pila. 
\\ Sia $P=(Q,\Sigma, \Gamma, \delta, q_0, Z_0, F)$ un PDA, allora $\forall w \in \Sigma^*, \beta \in \Gamma^*: (p,\alpha) \in \delta(q,a,X) \Rightarrow (q, aw, X\beta) \vdash (p,w,\alpha, \beta)$
\\ Il simbolo $\vdash$ significa "deduzione logica", mentre definiamo $\overset{*}{\vdash}$ come la chiusura transitiva di $\vdash$ 

	\begin{figure}[ht]
		\includegraphics[scale = 0.5]{media/diagramma_pda.png}
		\centering
    \caption{Diagramma PDA}
	\end{figure}

\subsection{Accettazione per stato finale} 
Sia $P=(Q,\Sigma, \Gamma, \delta, q_0, Z_0, F)$ un PDA, il \emph{linguaggio accettato} da P per stato finale è: 
\[L(P)=\{w:(q_0,w,Z_0) \overset{*}{\vdash}(q,\epsilon,\alpha, q \in F)\}\]
In altre parole una volta che $w$ è in uno stato accettante il contenuto della pila è irrilevante

\subsection{Accettazione per pila vuota} 
Sia $P=(Q,\Sigma, \Gamma, \delta, q_0, Z_0, F)$ un PDA, il \emph{linguaggio accettato} da P per pila vuota è: 
\[N(P)=\{w:(q_0,w,Z_0) \overset{*}{\vdash}(q,\epsilon,\epsilon)\}\]
NB $q$ è un generico stato, dunque $N(P)$ è degli input $w$ che svuotano la stack

\newpage
\subsection{Da stack vuota a stato finale} 
Dimostriamo ora che per pila vuota o per stato finale sono linguaggi equivalenti. 
\\ \textbf{Teorema}: Se $L=N(P_N)$ per un PDA $P=(Q,\Sigma, \Gamma, \delta, q_0, Z_0)$ allora $\exists$ PDA $P_F$ tale che $L=N(P_F)$.
\\ \textbf{A parole}: L'idea è avere un simbolo nuovo $X_0$ che specifica quando arrivano allo stato finale sia $P_F$ che $P_N$. $p_0$ ci server per inserire il primo simbolo nello stack ed andare in $q_0$. Andiamo in $p_F$ quando $P_N$ svuota la stack. 
\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/PNPF.png}
	\centering
\end{figure}
\\
\textbf{Dimostrazione} Sia: 
\[P_F = \{Q \cup \{p_0, p_f\}, \Sigma, \Gamma \cup \{ X_0\}, \delta_F, p_0, X_0, \{p_f\}\}\]
dove $\delta$ è definita come: 
\begin{enumerate}
  \item $\delta(p_0,\epsilon, X_0)=\{(q_0,Z_0, X_0)\}$
    transazione spontanea da $P_F$ a $P_N$
  \item $\forall q \in Q,a \in \Sigma \cup \{\epsilon\}, Y \in \Gamma: \delta_F(q,a,Y) =\delta_N(q,a,Y)$
  \item $\delta_F(q,a,Y)$ contiene $(p_j, \epsilon)$ per ogni $q$ in $Q$
\end{enumerate}

\newpage
Consideriamo un automa \emph{if} \emph{else} in C, dobbiamo leggere tanti if quanti else quindi usiamo Z per contare la differenza tra i 2 simboli.

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/if_else.png}
	\centering
\end{figure}

Leggendo i inseriamo una Z, leggendo e rimuoviamo una Z.Formalmente: 
\[ P_N = (\{q\}, \{i,e\}, \{Z\}, \delta_N,q,Z) \]
dove $\delta$: 
\begin{enumerate}
  \item $\delta(q,i,Z) = \{(q,ZZ)\}$ leggo i aggiungo Z 
  \item $\delta(q,e,Z) = \{(q,\epsilon)\}$ leggo e rimuovo Z
\end{enumerate}
A partire da $P_N$ costruiamo $P_F$:
\[ P_F = (\{p,q,r\}, \{i,e\}, \{Z,X_0\}, \delta_F,p,X_0, \{r\}) \]
dove $\delta$:
\begin{enumerate}
  \item $\delta_F(p,\epsilon, X_0) = \{(q,ZX_0)\}$ simulo lettura primo simbolo, $X_0$ va in fondo allo stack
  \item $\delta_F(q, i, Z) = \{(q,ZZ)\}$ leggo i aggiungo Z
  \item $\delta_F(q, e, Z) = \{(q,\epsilon)\}$ leggo e rimuovo z
  \item $\delta_F(q, \epsilon, X_0) = \{(r,\epsilon)\}$ $P_F$ accetta quando $P_N$ si svuota
\end{enumerate}

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/if_else_pf.png}
	\centering
  \caption{Diagramma P\textsubscript{F}}
\end{figure}

\subsection{Da stato finale a pila vuota}
Facciamo il percorso inverso $P_F \rightarrow P_N$. Si aggiunge un $\epsilon$-transizione da ogni stato accettante di $P_F$ a un nuovo stato p. Quando siamo in p, consumo lo stack senza leggere input. 
\\ Per evitare di svuotare lo stack per stringhe non valide uso $X_0$ come indicatore di fondo. Il nuovo stato $p_0$ serve solo ad arrivare allo stato iniziale $q_0$ e mettere $X_0$ in fondo allo stack. 
\\ \textbf{Teorema}: Se $L=N(P_F)$ per un PDA $P_F=(Q,\Sigma, \Gamma, \delta_F, q_0, Z_0, F)$ allora $\exists$ PDA $P_N$ tale che $L=N(P_N)$.
\\ \textbf{Dimostrazione}: 
\[ P_N = (Q \cup \{p_0, p\}, \Sigma,\Gamma \cup \{X_0\}, \delta_N,p_0,X_0) \]
dove $\delta_N$ è:
\begin{enumerate}
  \item $\delta_N(p,\epsilon, X_0) = \{(q,ZX_0)\}$ simulo lettura primo simbolo, $X_0$ va in fondo allo stack
  \item $\forall \; q$ in $Q$, $a$ in $\Sigma$ e ogni $Y$ in $\Gamma$: $\delta_N(q,a,Y)$ contiene tutte le coppie  in $\delta_F(q,a,Y)$. Ovvero $P_N$ simula $P_F$.
  \item $\forall \; q$ in $F$ e ogni $Y$ in $\Gamma$: $\delta(q,a,Y)$ contiene $(p,\epsilon)$. Ogni volta che $P_F$ accetta $P_N$ può svuotare lo stack
  \item $\forall \; Y$ in $\Gamma$: $\delta(q,a,Y) = \{p, \epsilon\}$ quando arrivo a $p$, quindi $P_F$ ha accettato, $P_N$ svuota lo stack senza leggere input
\end{enumerate}

\begin{figure}[ht]
  \includegraphics[scale = 0.4]{media/cfg_pda.png}
  \centering
\end{figure}

\newpage
\subsection{Equivalenza tra PDA e CFG} 
Un linguaggio è
generato da una CFG
se e solo se è
accettato da un PDA per pila vuota
se e solo se è
accettato da un PDA per stato finale

\subsection{Da CFG a PDA}
Data una CFG costruiamo una PDA che simula la derivazione a sinistra. Ogni espressione non terminale si può scrivere come $xA\alpha$, dove $A$ è la variabile più a sinistra, $x$ sono gli simboli terminali a sinistra di $A$ e $\alpha$ sono le variabili alla destra di $A$.
\\ 
Chiamiamo $A\alpha$ la coda della forma/espressione. Una coda di terminali è da considerarsi $\epsilon$. Dobbiamo simulare un PDA, dove accettiamo una stringa terminale $w$. La coda di $xA\alpha$ compare sullo stack con A in cima, $x$ è quello che consumiamo per aggiungere una stack, $w=xy$.
\\ Quindi data la transazione $(q,y,A\alpha)$, che rappresenta $xA\alpha$, l'automa sceglie di espandere $A \rightarrow \beta$. $\beta$ va in cima allo stack entrando nella ID $(q,y,\beta \alpha)$, il PDA ha un unico stato $q$.
\\ Non è detto però che $\beta$ sia terminale, e potrebbe avere dei terminali che lo precedono, quindi dobbiamo eliminare ogni terminale all'inizio di $\beta \alpha$. Confronto i terminali con i successi input per verificare correttezza, altrimenti il processo muore. 
\\ Nel caso in cui tutto vada bene, lo stack è vuoto e abbiamo la $w$ corretta. 
\\ Formalmente: Sia $G=(V,T,Q,S)$ una CFG, costruiamo il PDA $P$ che accetta $L(G)$ per stack vuoto: 
\[ P = (\{q\},T,V \cup T, \delta, q, S) \]
dove $\delta$ è definita come segue 
\[\delta(q,\epsilon,A) = \{(q,\beta): A \rightarrow \beta \in Q\}\]
per ogni terminale a, $\delta(q,a,a) = \{(q, \epsilon)\}$
\\ \textbf{Esempio:} 
\\ Considero la grammatica $S \rightarrow \epsilon|SS|iS|iSe$. Il PDA corrispondente è: 
\[ P = (\{q\}, \{i,e\}, \{S,i,e\},\delta,q,S)\]
dove $\delta(q,\epsilon,S)=\{(q,\epsilon),(q,SS),(q,iS),(q,iSe)\}, \delta(q,i,i)=\{(q,\epsilon)\} 
\\ \delta(q,e,e)=\{(q,\epsilon)\}$.

\subsection{Da PDA a CFG}
Per ogni PDA $P$ possiamo costruire un CFG $G$ il cui linguaggio coincide con quello accettato dallo stack vuoto di $P$. Dobbiamo prendere atto del momento più importante di una PDA, ovvero quando viene fatto il pop di un elemento dello stack, perché è cambiato lo stato. Nel nostro caso server tenere traccia dello stato passato. Ogni stato che cambia "scendo" nello stack.

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/grafico_pda.png}
	\centering
\end{figure}
Nel grafico si può notare che ogni eliminazione $Y_1,Y_2,...,Y_k$ coincide con la lettura di un input $x$. Non importa quanti passaggi siano stati fatti, qua viene visto lo step finale, quando $Y$ esce.
\\ Inoltre viene visto anche il passaggio di stato, che anche esso coincide con l'eliminazione.
\\ Per costruire una CFG a partire dal PDA usiamo variabile che rappresentano un "evento" con due parti: 
\begin{enumerate}
  \item l'eliminazione definitiva dallo stack di $X$
  \item il passaggio da $p$ a $q$, dopo che scambio $X$ con $\epsilon$ sullo stack
\end{enumerate}
Questa variabile la rappresentiamo come $[pXq]$. 
\\ \textbf{Formalmente}: Sia $P=(Q,\Sigma,\Gamma, \delta,q_0,Z_0)$ un PDA. \\ Definiamo $G=(V,\Sigma,R,S)$ con:
\begin{itemize}
  \item $V=\{[pXq]:\{p,q\} \subseteq Q,X \in \Gamma\} \cup \{S\}$ ovvero V contiene il simbolo iniziale $S$ e tutti i simboli in $Q$ e in $\Gamma$
  \item $R= \{S \rightarrow [q_0 Z_0 p]:p \in Q \} \cup$ \\ $\{q X r_k] \rightarrow a[r Y_1 r_1]...[r_{k-1} Y_k r_k]:$
  \\ $a \in \Sigma \cup \{\epsilon\}$,
  \\ $\{r_1,...,r_k\} \subseteq Q$, 
  \\ $(r,Y_1 Y_2...Y_k) \in \delta(q,a,X)\}$
  \begin{itemize}
      \item $[q_0 Z_0 p]$ genera tutte le stringhe $w$ e passa dallo stato $q_0$ a $p$. Al termine dell'operazione lo stack P sarà svuotato
      \item $\{q X r_k] \rightarrow a[r Y_1 r_1]...[r_{k-1} Y_k r_k]:$ è una produzione che indica il passaggio $q$ a $r_k$ e il susseguirsi di passaggi per arrivarci. Al primo passaggio leggo l'input $a$ e itero k volte finché non ho eliminato Y. $a$ può essere $\epsilon$
        \item se k=0 allora $Y_1 Y_2 ... Y_k = \epsilon$ e $r_k = r$
  \end{itemize}
\end{itemize}
\textbf{Esempio}: Convertiamo

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/if_else.png}
	\centering
\end{figure}
\[ P_N = (\{q\}, \{i,e\}, \{Z\}, \delta_N,q,Z) \]
dove 
\begin{itemize}
  \item $\delta(q,i,Z) = \{(q,ZZ)\}$ 
  \item $\delta(q,e,Z) = \{(q,\epsilon)\}$ 
\end{itemize}
in una grammatica 
\[G=(V,\{i,e\},R,S)\]
dove 
\begin{itemize}
  \item $V=\{[qZq],S\}$
  \item $R=\{S \rightarrow [qZq],[qZq] \rightarrow i[qZq[qZq], [qZq] \rightarrow e \}$
\end{itemize}
Per semplicità $[qZq]=A$ quindi $S \rightarrow A$ e $A=iAA|e$

\textbf{Esempio}: Convertiamo 
\[ P = \{p,q\}, \{0,1\},\{X,Z_0\},\delta, q, Z_0) \]
dove $\delta$ è data da: 
\begin{enumerate}
  \item $\delta(q,1,Z_0) = \{(q,XZ_0)\}$
  \item $\delta(q,1,X) = \{(q,XX)\}$
  \item $\delta(q,0,X) = \{(p,X)\}$
  \item $\delta(q,\epsilon,X) = \{(q,\epsilon)\}$
  \item $\delta(p,1,X) = \{(p,\epsilon)\}$
  \item $\delta(p,0,Z_0) = \{(q,Z_0)\}$
\end{enumerate}
in una CFG.
\\ Otteniamo $G=(V,\{0,1\},R,S$, dove 
\[ V = \{[qZ_0q],[pZ_0q],[qZ_0p],[pZ_0p], [qXq],[pXq],[qXp],[pXp],\} \]
e le produzioni in R 
\[ S \rightarrow [q Z_0 q] [qZ_0p]\]
Dalla transizione (1) $\delta(q,1,Z_0) = \{(q,XZ_0)\}$ si ha:
\\ $[q Z_0 q] \rightarrow 1[qXq][qZ_0 q]$
\\ $[q Z_0 q] \rightarrow 1[qXp][pZ_0 q]$
\\ $[q Z_0 p] \rightarrow 1[qXq][qZ_0 p]$
\\ $[q Z_0 p] \rightarrow 1[qXp][pZ_0 p]$
\vspace{2mm}
\\ Dalla transizione (2) $\delta(q,1,X) = \{(q,XX)\}$
\\ $[qXq] \rightarrow 1[qXq][qXq]$
\\ $[qXq] \rightarrow 1[qXp][pXq]$
\\ $[qXp] \rightarrow 1[qXq][qXp]$
\\ $[qXp] \rightarrow 1[qXp][pXp]$
\vspace{2mm}
\\ Dalla transizione (3) $\delta(q,0,X) = \{(p,X)\}$
\\ $[qXq] \rightarrow 0[pXq]$
\\ $[qXp] \rightarrow 0[pXp]$
\vspace{2mm}
\\ Dalla transizione (4) $\delta(q,\epsilon,X) = \{(q,\epsilon)\}$
\\ $[qXq] \rightarrow \epsilon$
\vspace{2mm}
\\ Dalla transizione (5) $\delta(p,1,X) = \{(p,\epsilon)\}$
\\ $[pXp] \rightarrow 1$
\vspace{2mm}
\\ Dalla transizione (6) $\delta(p,0,Z_0) = \{(q,Z_0)\}$
\\ $[pZ_0 q] \rightarrow 0[qZ_0 q]$
\\ $[pZ_0 p] \rightarrow 0[qZ_0 p]$


\section{PDA deterministici}
I PDA deterministici sono quelli usati per i parser, sono un sotto caso utile per noi.
\\ 
Un PDA deterministico non deve avere mosse alternative, se $\delta(q,a,X)$ ha più di una coppia sicuramente non è deterministico. Questo però non è sufficiente però, perché potresti avere due prodotti diversi e bisogna scegliere.
\\ Definiamo quindi un PDA $P=(Q, \Sigma, \Gamma, \delta, q_0, Z_0, F)$ deterministico se e solo se: 
\begin{enumerate}
  \item $\delta(q,a,X)$ ha al massimo un elemento $\forall q \in Q, a \in \Sigma, X \in \Gamma$
  \item Se $\delta(q,a,X)$ non è vuoto per un $a \in \Sigma$ allora $\delta(q,\epsilon,X)$ deve essere vuoto
\end{enumerate}

\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/pda_det.png}
	\centering
  \caption{Esempio di PDA deterministico (DPDA)}
\end{figure}

\newpage
\subsection{DPA che accettano per stato finale} 
Regex $\subset$ L(DPDA) $\subset$ CFL.
\\ \textbf{Teorema} Se L è regolare allora L=L(P) per un qualsiasi DPDA P.
\\ \textbf{Prova}: Dato che L è regolare $\exists$ DFA $A$ tale che L=L(A)
\[A=(Q,\Sigma, \delta_A, q_0, F)\]
definiamo il DPDA
\[P=(Q,\Sigma,\{Z_0\},\delta_P,q_0,Z_0,F)\]
dove
\[\sigma_P(q,a,Z_0) = \{(\delta_A(q,a),Z_0\}\]
$\forall \; p,q \in Q$ e $a \in \Sigma$
\\ applichiamo un induzione su $|w|$
\[(q_0,w,Z_0) \overset{*}{\vdash} (p,\epsilon, Z_0) \iff \circumdelta(q_0,w)=p\]

\subsection{DPDA che accettano per la pila vuota}
Si usa la \textbf{proprietà del prefisso}: un linguaggio ha la proprietà del prefisso se NON esistono due stringhe uguali una che è il prefisso dell'altra. 
\\ $L_{pal}$ ha la proprietà del prefisso
\\ $\{0\}^*$ non ha la proprietà del prefisso
\\ \textbf{Teorema} L è N(P) per un qualche DPDA P se e solo se L ha la proprietà del prefisso e L è $L(P')$ per qualche DPDA $P'$

\subsection{DPDA e non ambiguità}
L(DPDA) non per forza coincide con CFL non ambigue, viceversa invece è vero. 
\begin{theorem}
Se $L=N(P)$ per qualche DPDA P, allora L ha una grammatica non ambigua 
\label{dpda1}
\end{theorem}
\textbf{Dimostrazione} stessa dimostrazione che fai da PDA a CFG, solo che parti con un DPDA
\vspace{2mm}
\\ L'enunciato può essere rafforzato 
\begin{theorem}
 Se $L=L(P)$ per qualche DPDA P, allora L ha una grammatica non ambigua 
 \label{dpda2}
\end{theorem}
\textbf{Dimostrazione}
\\ Sia \$ un simbolo non presente in $L$, e sia $L'=L\$$. In altri termini L' sono le stringhe L seguite da \$.
Dal Teorema. \ref{dpda1} L' ha la proprietà di prefisso.
Dal Teorema. \ref{dpda2} esiste una grammatica $G'$ che genera un linguaggio $N(P')$ che è $L'$.
\\ Costruiamo quindi una grammatica $G$ tale che $L(G)=L$, dobbiamo solo togliere \$ dalla fine delle stringhe, così che $G'$ coincida con $G$. Introduciamo \$ in $G$
\[ \$ \rightarrow \epsilon \]
Visto che $L(G')=L'$ consegue che $L(G)=L$. $G$ non è ambigua.

\section{Proprietà di CFG}
Elenco delle proprietà: 
\begin{itemize}
  \item \emph{Semplificazione} di una CFG. Una CFL ha una grammatica in forma speciale
  \item \emph{Pumping Lemma} per CFG, simile alle regex
  \item \emph{Proprietà di chiusura}.
  \item \emph{Proprietà di decisione}. Verifichiamo l'appartenenza e l'essere vuoto 
\end{itemize}

\subsection{Forma normale di Chomsky}
Ogni CFL (senza $\epsilon$) è generato da una CFG dove tutte le forme sono 
\[A \rightarrow BC, A \rightarrow a\]
dove $A,B,C$ sono variabili, mentre $a$ è un terminale. Questa è detta forma di Chomsky e per ottenerla dobbiamo pulire la grammatica: 
\begin{itemize}
  \item Eliminare i \emph{simboli inutili}, ovvero ogni simbolo che non appartiene a $S \overset{*}\Rightarrow w$
  \item Eliminare le \emph{produzioni} $\epsilon$, dalla forma $A \rightarrow \epsilon$
  \item Eliminare le \emph{produzioni unità} ovvero le produzioni $A \rightarrow B$
\end{itemize}

\subsection{Eliminazione di simboli inutili} 
Un simbolo $X$ è \emph{utile} per una grammatica $G=(V,T,P,S)$ se esiste una derivazione 
\[ S \xRightarrow[G]{*} \alpha X \beta \xRightarrow[G]{*} w\]
per una stringa $w \in T^*$. Possiamo eliminare i simboli inutili senza cambiare il significato della grammatica. Un simbolo utile deve avere 2 caratteristiche: 
\begin{enumerate}
  \item $X$ è un \emph{generatore} se esiste una stringa w tale che $X \xRightarrow[]{*} w$. Ogni terminale è un generatore di se stesso in 0 passi
  \item $X$ è \emph{raggiungibile} se esiste una derivazione $S \xRightarrow[]{*} \alpha X \beta$ per qualche $\{\alpha, \beta\} \subseteq (V \cup T)^*$
\end{enumerate}
Un simbolo deve essere sia raggiungibile che generatore, eliminando prima i non generatori e poi i non raggiungibili abbiamo una grammatica solo di simboli utili.
\\ \textbf{Esempio}: Consideriamo la grammatica 
\[ S \rightarrow AB | a \]
\[ A \rightarrow b \]
A genera b, S genera A, B non è un simbolo generatore. Eliminando B: 
\[ S \rightarrow a \]
\[ A \rightarrow b \]
Adesso solo $a$ è raggiungibile quindi 
\[ S \rightarrow a \]
che è la nostra grammatica iniziale
\\ 
NB se elimino prima i simboli non raggiungibili non cambia nulla, tutto è raggiungibile
\[ S \rightarrow AB | a \]
\[ A \rightarrow b \]
ora elimino $B$ 
\[ S \rightarrow a \]
\[ A \rightarrow b \]
e ho una grammatica con simboli inutili

\subsection{Eliminazione delle produzioni epsilon}
Se L è un CFL, allora L \{$\epsilon\}$ ha una grammatica priva di produzioni $\epsilon$.
\\ La variabile A è annullabile se $A \xRightarrow[]{*}\epsilon$
\\ Sia a annullabile, rimpiazzo 
\[ B \rightarrow \alpha A \beta\]
\\ con
\[ B \rightarrow \alpha \beta\]
indico con n(G) l'insieme dei simboli annullabili della grammatica $G=(V,T,P,S)$.
\\ \textbf{Esempio} Sia la grammatica
\[ S \rightarrow AB, A \rightarrow aAA|\epsilon, B \rightarrow bBB|\epsilon\]
Abbiamo $n(G)=\{A,B,S\}$, la prima regola diventa 
\[ S \rightarrow AB| A |B \]
la seconda regola diventa
\[ A \rightarrow aAA| aA | aA |a \]
la terza regola diventa
\[ B \rightarrow bBB| bB | bB |b \]
La nuova grammatica sarà: 
\[ S \rightarrow AB| A |B 
A \rightarrow aAA| aA |a
B \rightarrow bBB| bB |b
\]

\subsection{Eliminazione produzioni unità}
\[A \rightarrow B \]
è una produzione unità nel caso in cui A e B siano variabili. Le produzioni variabili sono eliminabili.
\\ Data la grammatica 
\\ $ E \rightarrow T | E + T$
\\ $ T \rightarrow F | T * F$
\\ $ F \rightarrow I | (E) $
\\ $ I \rightarrow a | b | Ia | Ib | I0 | I1 $
\\ le produzioni unità sono $E \rightarrow T, T \rightarrow F, F \rightarrow I$ 
\\ Considero la produzione $E \rightarrow T$ e la espando 
\[ E \rightarrow F, E \rightarrow T * F \]
poi espando $E \rightarrow F$
\[ E \rightarrow I, (E), T * F \]
Infine espando $E \rightarrow I$
\[ E \rightarrow a | b| Ia |Ib|I0|I1|(E)|T*F\]
lo stesso applico lo stesso procedimento con $T \rightarrow F$ e $F \rightarrow I$: 
\[ T \rightarrow a | b| Ia |Ib|I0|I1|(E)\]
\[ F \rightarrow a | b| Ia |Ib|I0|I1\]
la nuova grammatica sarà 
\\ $ E \rightarrow a | b | Ia | Ib | I0 | I1  | (E) | T*F | E+T $
\\ $ T \rightarrow a | b | Ia | Ib | I0 | I1  | (E) | T * F$
\\ $ F \rightarrow a | b | Ia | Ib | I0 | I1  | (E) $
\\ $ I \rightarrow a | b | Ia | Ib | I0 | I1  $
\\ Questo procedimento non funziona nel caso in cui ci siano dei cicli. Se trovo un unità già espansa la rimuovo.
\vspace{2mm}
\\ \textbf{Esempio} Si consideri la grammatica
\\ $ A \rightarrow B | a$
\\ $ B \rightarrow C | b$
\\ $ C \rightarrow A | c$
\\ Espando $ A \rightarrow B$
\\ Da $ A \rightarrow B $ ottengo 
\[ A \rightarrow C | b \]
\\ Da $ A \rightarrow C $ ottengo 
\[ A \rightarrow A | c | b \]
\\ Da $ A \rightarrow A $ ottengo 
\[ A \rightarrow B \; a | c | b \]
Qui mi fermo perché tornei a fare le stesse produzioni, mi rimane:
\[ A \rightarrow a | c | b \]
eseguo per le altre 3 e la nuova grammatica risulta 
\[ A \rightarrow a | b | c \]
\[ B \rightarrow a | b | c \]
\[ C \rightarrow a | b | c \]

\subsection{Sommario} 
Per pulire una grammatica bisogna: 
\begin{enumerate}
  \item Eliminare le produzioni $\epsilon$
  \item Eliminare le produzioni unità
  \item Eliminare simboli inutili
\end{enumerate}
in quest'ordine

\subsection{Forma normale di Chomsky, CNF}
Ogni CNF non vuoto, senza $\epsilon$, ha una grammatica $G$ di simboli inutili nella seguente forma 
$A \rightarrow BC$ dove $\{A,B,C\} \subseteq V$ oppure
$A \rightarrow a$ dove $a\in T$ e $A \in V$.
\\ Per arrivare a questa forma bisogna: 
\begin{itemize}
  \item Pulire la grammatica 
  \item Modificare le produzioni con 2 o più simboli in modo che siano tutte variabili 
  \item Ridurre le produzioni con più di 2 simboli in catene da 2 simboli
\end{itemize}
Quindi per ogni terminale $a$ più lungo di 1 creo una nuova variabile $A \rightarrow a$.
Mentre per ogni regola 
\[ A \rightarrow B_1,B_2,...,B_k \]
$k \geq 3$ creo nuove variabili $C_1, C_2, ..., C_{k-2}$
\[ A \rightarrow B_1, C_1 \]
\[ C_1 \rightarrow B_2, C_2 \]
\[...\]
\[ C_{k-3} \rightarrow B_{k-2}, C_{k-2} \]
\[ C_{k-2} \rightarrow B_{k-1}, B_k \]
\textbf{Esempio}
Usiamo la grammatica 
\\ $E\rightarrow | E+F | T*F | (E) | b | Ia | Ib | I0 | I1   $
\\ $T\rightarrow (E) | T*F | a | b | Ia | Ib | I0 | I1$
\\ $F\rightarrow (E)| a | b | Ia | Ib | I0 | I1   $
\\ $I\rightarrow a | b | Ia | Ib | I0 | I1  $
\vspace{5mm}
\\ applichiamo il passo 2
\\ $A \rightarrow a, B \rightarrow b, Z \rightarrow 0, O \rightarrow 1 $
\\ $P \rightarrow +, M \rightarrow *, L \rightarrow (, R \rightarrow ) $
\vspace{5mm}
\\ otteniamo 
\\ $E\rightarrow EPF | TMF | LER | b | IA | IB | IZ | IO   $
\\ $T\rightarrow LER | TMF | a | b | IA | IB | IB | IO$
\\ $F\rightarrow LER | a | b | IA | IB | IZ | IO$
\\ $I\rightarrow a | b | IA | IB | IZ | IO $
\\ $A \rightarrow a, B \rightarrow b, Z \rightarrow 0, O \rightarrow 1 $
\\ $P \rightarrow +, M \rightarrow *, L \rightarrow (, R \rightarrow ) $
\vspace{5mm}
\\ Applichiamo il passo 3
\\ $E \rightarrow EPT$ diventa $E \rightarrow EC_1, C_1 \rightarrow PT$
\vspace{2mm}
\\ $E \rightarrow TMF, T \rightarrow TMF$ diventa $E \rightarrow TC_2,T \rightarrow TC_2, C_2 \rightarrow MF$
\vspace{2mm}
\\ $E \rightarrow LER, T \rightarrow LER, F \rightarrow LER$ diventa $E \rightarrow LC_3,T \rightarrow TC_3, F \rightarrow LC_3, C_3 \rightarrow ER$
\vspace{5mm}
\\ La grammatica finale diventa 
\\ $E\rightarrow EC_1 | TC_2 | LC_3 | b | IA | IB | IZ | IO   $
\\ $T\rightarrow LC_3 | TC_2 | a | b | IA | IB | IB | IO$
\\ $F\rightarrow LC_3 | a | b | IA | IB | IZ | IO$
\\ $I\rightarrow a | b | IA | IB | IZ | IO $
\\ $C_1 \rightarrow PT$, $C_2 \rightarrow MF$, $C_3 \rightarrow ER$
\\ $A \rightarrow a, B \rightarrow b, Z \rightarrow 0, O \rightarrow 1 $
\\ $P \rightarrow +, M \rightarrow *, L \rightarrow (, R \rightarrow ) $

\subsection{Pumping lemma per CFL}
\textbf{Pumping lemma per i linguaggi regolari}: per una stringa sufficiente lunga è possibile causare un ciclo e creare un infinità di stringhe che appartengono al linguaggio 
\\ \textbf{Pumping lemma per CFL}: per una stringa sufficiente lunga è possibile trovare due sottostringhe vicine che si possono iterare "in tandem". Posso iterare $i$ volte per trovare nuove stringhe appartenenti al linguaggio.
\\ \textbf{Formalmente}: 
\\ Sia L un CFL. Allora $\exists n \geq 1$ che soddisfa: ogni $z \in L: |z| \geq n$ è composto da 5 stringhe $z=uvwxz$ tali che: 
\begin{enumerate}
  \item $|vwx| \leq n$
  \item $|vx| > 0$
  \item $\forall i \geq 0, uv^i wx^i y \in L$
\end{enumerate}
\textbf{Dimostrazione}: 
\begin{itemize}
  \item Si consideri una grammatica per $ L \backslash \{\epsilon\}$ in CNF
  \item Assumiamo che la grammatica abbia $m$ variabili, con $n=2^m$
  \item Sia $z \in L$ una qualsiasi stringa tale che $|z| \geq 2^m$ allora ogni albero sintattico di $z$ ha un cammino $\geq m + 1$
  \item Se tutti i cammini dell'albero hanno lunghezza $\leq m$ allora la stringa generata ha lunghezza $\leq 2^{m-1}$
\end{itemize}
Prendendo un cammino sufficientemente lungo, a partire da $A$ fino a $A_0$ fino a $A_k$ a un certo punto troverò due sotto alberi $A_i, A_j$ con $i\neq j$ che sono uguali.
Questo succede perché $k \geq m$, quindi avendo solo $m$ variabili distinte è normale che si ripetano.
Ora posso radicare l'albero $A_j$ sotto $A_i$ e la stringa finale non cambia, perché rispetta $z=uv^i wx^i y$
\begin{figure}[ht]
	\includegraphics[scale = 0.4]{media/tree.png}
	\centering
\end{figure}
Notiamo che: 
\begin{itemize}
  \item l'albero in $A_i$ ha altezza $\leq m+1$ quindi la stringa corrispondente ha lunghezza $\leq 2^m = n$ e quindi $(|vwx| \leq n)$
  \item $v$ e $x$ non possono essere vuote, perché la grammatica genera variabili non terminali perciò $|vx| >0$
  \item posso ripetere l'albero $A_i$ $n$ volte e creo sempre una stringa valida ($uv^i wx^i y \in L$)
\end{itemize}

\subsection{Applicazione Pumping lemma}
Possiamo usarlo per dimostrare che un linguaggio non è libero.
\vspace{2mm}
\\ \textbf{Esempio}: Si consideri $L={0^m 10^m 10^m :m \geq 1}$, dimostra che $L$ non è CFL.
\\ \textbf{Dimostrazione}: Assumiamo, per assurdo, che L sia CFL. Sia $n$ la costante del Pumping lemma. Si consideri la strina $z=0^n 10^n 10^n$. Si ha che $z \in L$ e $|z| \geq n$. Allora per il Pumping lemma, $z=uvwxy$ con $|uwx| \leq n$, $|vx| >0$ e $uv^i wx^i y \in L$ per ogni $i \geq 0$. Consideriamo i seguenti casi: 
\begin{itemize}
  \item $vx$ contiene almeno un 1, la catena può essere replicata all'infinito quindi sicuramente per $i>=2$ $|uwx| \notin L$
  \item $vx$ contiene almeno un 0, in questo caso non viene rispettata $0^n$ perché sia se $vx$ forma un gruppo di 0, sia se ne forma 2, andrà in ogni caso a ridurre il numero di 0 totali
\end{itemize}
In entrambi i casi $uwy \notin L$
\vspace{2mm}
\\ \textbf{Esempio 2}: Si consideri $L={0^{k^2}:k>1}$, dimostra che $L$ non è CFL.
\\ \textbf{Dimostrazione}: Assumiamo, per assurdo, che L sia CFL. Sia $n$ la costante del Pumping lemma. Si consideri la strina $z=0^{n^2}$. Si ha che $z \in L$ e $|z| \geq n$. Allora per il Pumping lemma, $z=uvwxy$ con $|uwx| \leq n$, $|vx| >0$ e $uv^i wx^i y \in L$ per ogni $i \geq 0$. Consideriamo il seguente caso:
\\ $uv^2wx^2y \in L$ per Pumping Lemma
\\ (Presumo che qua stiamo contando il numero di 0) $n^2 < |uv^wx^2y| \leq n^2+n < n^2 + 2n + 1 = (n+1)^2$. Non esiste un quadrato perfetto tra $n^2$ e $(n+1)^2$, il numero di 0 non torna, quindi $uv^2wx^2y \notin L$.

\subsection{Proprietà di chiusura dei CFL}
\begin{theorem}
    I CFL sono chiusi rispetto ai seguenti operatori: unione, concatenazione e chiusura di Kleene (*) e chiusura positiva +
\end{theorem}
\vspace{2mm}
\textbf{Theorem:} se L è CFL allora lo è anche $L^R$
\\ \textbf{Dimostrazione:} Supponiamo che L sia generato da $G=(V,T,P,S)$. Costruiamo $G^R=(V,T,P^R,S)$ dove 
\[ P^R = \{ A \rightarrow \alpha^R : A \rightarrow \alpha \in P\} \]
\\ Si può dimostrare per induzione che $(L(G))^R = L(G^R)$

\subsection{I CFL NON sono chiusi rispetto all'intersezione}
Sia $L_1 = \{ 0^n1^n2^i:n \geq 1, i \geq i \}$. Allora $L_1$ è CFL con grammatica
\[ S \rightarrow AB \]
\[ A \rightarrow 0A1|01 \]
\[ B \rightarrow 2B|2 \]
Inoltre $L_2 = \{ 0^i1^n2^n:n \geq 1, i \geq i \}$. Allora $L_1$ è CFL con grammatica
\[ S \rightarrow AB \]
\[ A \rightarrow 0A0|0 \]
\[ B \rightarrow 1B2|12 \]
Invece, $L_1 \cap L_2 = \{ 0^n1^n2^n:n \geq 1 \} $ non è CFL per pumping lemma.

\subsection{Operazioni su linguaggi liberi e regoli}
\begin{theorem}
    Se L è CFL e R è regolare, allora L $\cup$ R è CFL
\end{theorem}
\textbf{Prova:} Sia L che accetta PDA 
\[P=(Q_P, \Sigma, \Gamma, \delta_P, q_P, Z_0, F_P)\]
per stato finale, e sia R accettato dal DFA 
\[ A=(Q_A,\Sigma,\delta_A,q_A,F_a) \]
\newpage 
Costruiamo un PDA per $L \cup R$: 

\begin{figure}[ht]
	\includegraphics[scale = 0.3]{media/double_stack.png}
	\centering
	\caption{Semplice automa}
\end{figure}
Formalmente definiamo 
\[ P' = (Q_P \times Q_A, \Sigma, \Gamma, \delta, (q_P, q_A), Z_0, F_P \times F_A\]
dove 
\[\delta((q,p),a,X) = \{((r, \circumdelta,(p,a)),\gamma):(r,\gamma) \in \delta_P(q,a,X)\}\]
Per induzione si può provare per induzione su $\overset{*}{\vdash}$ che 
\[(q_P,w,Z_0)\overset{*}{\vdash}(q,\epsilon,\gamma) \; in \; P\]
se e solo se 
\[((q_P,q_A),w,Z_0) \overset{*}{\vdash} ((q, \circumdelta(q_A,w)),\epsilon,\gamma) \; in \; P'\]
\begin{theorem}
    Siano $L,L_1,L_2$ CFL e R regolare, Allora 
    \begin{itemize}
        \item $L \backslash R$ è CFL
        \item $\overline{L}$ è per forza CFL
        \item $L_1 \backslash L_2 $ non è per forza CFL
    \end{itemize}
\end{theorem}
\textbf{Prova:}
\begin{enumerate}
  \item $\overline{R}$ è regolare, $R \cap \overline{R}$ è CFL e $L\cap \overline{R}$ è CFL e $L\cap \overline{R} = L \backslash R$
  \item Se $\overline{L}$ fosse sempre CFL, seguirebbe che 
    \[ L_1 \cap L_2 = \overline{\overline{L_1} \cup \overline{L_2}}\]
    è sempre una CFL
  \item Notare che $\Sigma^*$ è CFL; quindi se $L_1 \backslash L_2$ è CFL allora lo è anche $\Sigma^* \backslash K = \overline{L}$
\end{enumerate}

\subsection{Proprietà di decisione per CFL}
Analizziamo i seguenti problemi decidibili:
\begin{itemize}
  \item Verificare che $L(G) \neq \emptyset$ per una CFG $G$
  \item Verificare che $w \in L(G)$, per una stringa $w$ ed una CFG G
\end{itemize}
E faremo un elenco di \textbf{problemi indecidibili}

\subsubsection{Verificare se un CFL è vuoto}
$L(G)$ è non vuoto se il simbolo iniziale $S$ è generante.
\\ Con un implementazione naive il tempo è $O(n^2)$, ottimizzando si passa $O(n)$

\subsubsection{Appartenenza a un CFL}
\[w \in L(G)?\]
\textbf{Tecnica inefficiente}
\\ Supponiamo che G sia in CNF, e che la stringa $w$ abbia lunghezza $|w|=n$, allora il suo albero binario ha al più $2n-1$ nodi.
\\ Genero tutti gli alberi sintattico di G e controllo se $w$ è in uno di quelli, tempo esponenziale in $n$

\subsubsection{Problemi indecidibili per CFL}
Elenco di problemi indecidibili: 
\begin{enumerate}
  \item Una data CFG $G$ è ambigua?
  \item Un dato CFL $L$ è inerentemente ambigua?
  \item L'intersezione di 2 CFL è vuota?
  \item 2 CFL sono uguali?
  \item Un CFL è universale?
\end{enumerate}

\section{Analisi Lessicale e sintattica}
There are 3 kinds of execution environment (language processor): 
\begin{itemize}
    \item Compilers: translate code in a forma that is executable by computers. More generally a compiler can \emph{translate} code into an equivalent code in another language. 

        Compilers also check for errors. If the compiled language is executable the users can add input and get output with the program.

        C and Java are compiled.
    \item Interpreters: Unlike a compiler the interpreter directly executes a program without producing an executable.

        Interpreters are slower than compilers, but can have better errors recognition since they execute the source statement by statement.

        Scheme, lisp and perl are interpreted

    \item Virtual machines: You can also combine the interpreter and the compiler. This is the case in Java where the code is begin compiled into \emph{bytecode} and executed by the JVM.

        The advantage is you can 2 machines, one for compiling and the other for interpreting thus speeding the interpretation process.

        Some java compilers translate the code \emph{just-in-time} (JIT) for the execution.
\end{itemize}

The whole process is the following: source -> preprocessor -> compiler -> assembler -> linker/loader.

\subsection{The compiler}
The operation of the compiler can be divided in 2 parts: \emph{analysis} and \emph{synthesis}. 

The analysis breaks the program into pieces and enforces grammatical structure.
If there are syntactic errors the user will get an error message.
If everything goes well then the program is translated it into a intermediate representation. 
It also creates a \emph{symbol table}, which will be used later.

The synthesis translates into the desired program, using the intermediate representation and the symbol table.  

Here's the steps:
\begin{itemize}
    \item Scanning (Lexical analysis)
    \item Parsing (Syntactic analysis)
    \item Type checking (Semantic analysis)
    \item Optimisation
    \item Code generation
\end{itemize}

\subsubsection{Lexical analysis}
Translates words into units. 

Ex. if x==y $\Rightarrow$ if, x, ==, y

\subsubsection{Parsing}
Once the lexical analysis is over, you can translate it into some kind of representation like \emph{diagram sentences}(it's just a tree).

\subsection{Building a lexer} 
Our input is this code snippet.
\begin{mycode}[]
    if(i==j): 
        z=0
    else 
        z=1
\end{mycode}
The compiler actually sees it as such:

\[\backslash tif (i == j)\backslash n\backslash t\backslash tz = 0;\backslash n\backslash telse\backslash n\backslash t\backslash tz = 1;\]
\\
We need to partition this string into substrings (lexemes) and classify them into role (tokens).
\\ 
Each lexeme has a name and value $<token name, value>$
\\
Therefore our input will be split. White spaces don't count and some special tokens will be grouped, line $\backslash$t means newline. 
\\ 
It's not a good idea to write a lexer by hand it's: tedious, error prone and non-maintainable. There are \emph{lexer generators} where you can define the lexemes and tokens and it will do the rest.
\\ 
This way we can focus on what the lexer should do rather than how. The what is done with declarative programming, while the how with imperative programming.
\\ 
First we build the lexer by hand in java, to get the feel of the code that will be hidden. 

\begin{table}[ht]
	\centering
    \begin{tabular}{ | c | c | }
        \hline
        ID & sequence of alphanumeric that start with a letter \\ 
        \hline
        EQUALS & "==" \\ 
        \hline
        PLUS & "+" \\ 
        \hline
        TIMES & "*" \\ 
        \hline
    \end{tabular}
	\caption{Esempio di tabella}
\end{table}

\begin{mycode}[title= Imperative lexer]
    c=nextChar();
    if (c == '=') { c=nextChar(); if (c == '=') {return EQUALS;}}
    if (c == '+') { return PLUS; }
    if (c == '*') { return TIMES; }
    if (c is a letter) {
    c=NextChar();
    while (c is a letter or digit) { c=NextChar(); }
    undoNextChar(c);
    return ID;
}
\end{mycode}
undoNextChar() determines if the next character is part of the lexeme or not. Each lexeme is usually partitioned at the maximum match, in order to avoid errors like "iffy" partition into "if" "fy" instead of the whole ID.
\\ 
Each comparison is the \emph{how}, but anything else 
is the \emph{what}.
We could abstract the how using automata. 
Each token is an automa or a regex, and the token reading is an automa responsible for the scanning.
\\ 
Let's define the automata: 
\begin{itemize}
    \item ID: [a-zA-Z][a-zA-Z0-9]*, ovvero legge una lettera e poi legge lettere o numeri
        \\
        \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
            \node[state, initial] (q0) {};
            \node[state, accepting] (q1) [right=of q0] {};
            \path[->]
                (q0) edge node {letter} (q1)
                (q1) edge[loop above] node {letter $|$ digit} (q1);
        \end{tikzpicture}
    \item EQUALS: ==
        \\
        \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
            \node[state, initial] (q0) {};
            \node[state] (q1) [right=of q0] {};
            \node[state, accepting] (q2) [right=of q1] {};
            \path[->]
                (q0) edge node {"="} (q1)
                (q1) edge node {"="} (q2);
        \end{tikzpicture}
    \item PLUS: +
        \\
        \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
            \node[state, initial] (q0) {};
            \node[state, accepting] (q1) [right=of q0] {};
            \path[->]
                (q0) edge node {"+"} (q1);
        \end{tikzpicture}
    \item TIMES: *
        \\
        \begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
            \node[state, initial] (q0) {};
            \node[state, accepting] (q1) [right=of q0] {};
            \path[->]
                (q0) edge node {"*"} (q1);
        \end{tikzpicture}
\end{itemize}

Now let's extend the FA to accept the whole input.
\\
\begin{tikzpicture}[shorten >=1pt,node distance=3.5cm,on grid,auto]
    \node[state, initial] (q0) {S};
    \node[state] (q1) [right=of q0]{};
    \node[state, accepting] (q2) [right=of q1] {};
    \path[->]
        (q0) edge node {letter} (q1)
        (q1) edge[loop above] node {letter $|$ digit} (q1)
        (q1) edge node[align=center] {any char except \\ letter or digit} (q2);
\end{tikzpicture}
\\
By the end of the operation we return a token and reset the automa to S.
\\ 
If we put everything together into a single automa: 
\\ 
\begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state] (q0) {S};
    \node[state] (q1) [below=of q0]{};
    \node[state] (f1) [below=of q1]{$F_1$};
    \node[state] (q2) [right=of q0]{ID};
    \node[state, accepting] (q3) [right=5.5cm of q2]{$F_2$};
    \node[state, accepting] (q4) [above=of q0]{$F_3$};
    \node[state, accepting] (q5) [left=of q0]{$F_4$};
    \path[->]
        (q0) edge node {letter} (q2)
        (q0) edge node {"+"} (q4)
        (q0) edge node {"*"} (q5)
        (q0) edge node {"="} (q1)
        (q1) edge node {"="} (f1)
        (q2) edge[loop above] node {letter $|$ digit} (q2)
        (q2) edge node[align=center] {any char except \\ letter or digit} (q3);
        ;
\end{tikzpicture}
\vspace{5mm}\\ 
Let's add the ASSIGN
\vspace{5mm}\\ 
\begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state] (q0) {S};
    \node[state] (q1) [below=of q0]{};
    \node[state] (assign) [below=of q2]{};
    \node[state] (f1) [below=of q1]{$F_1$};
    \node[state] (q2) [right=of q0]{ID};
    \node[state, accepting] (q3) [right=5.5cm of q2]{$F_2$};
    \node[state, accepting] (q4) [above=of q0]{$F_3$};
    \node[state, accepting] (q5) [left=of q0]{$F_4$};
    \path[->]
        (q0) edge node {letter} (q2)
        (q0) edge node {"="} (assign)
        (q0) edge node {"+"} (q4)
        (q0) edge node {"*"} (q5)
        (q0) edge node {"="} (q1)
        (q1) edge node {"="} (f1)
        (q2) edge[loop above] node {letter $|$ digit} (q2)
        (q2) edge node[align=center] {any char except \\ letter or digit} (q3);
        ;
\end{tikzpicture}
\\ 
At this point we need to know if the automa is not deterministic, then we convert it to a deterministic one. The only missing piece is remembering the position of the input, if the lexer errors we can return information about where.

Some practical concerns: 
\begin{itemize}
    \item Ambiguity: if the input is "if" is it an ID or a keyword? We can solve this by giving priority to the keywords, if has priority over ID.
    \item Discard white space: final state white space jumps to start
    \item Error inputs: discard illegal lexemes and return an error token, this token has the lowest priority
\end{itemize}
Automata are not the only way to build a lexer, we can also use regex.
The advantage is that regex are way easier to represent textually, thus being a better specification. 
Regex also are very compact.
\\ Let's define the regex for ID:
[a-zA-Z]. ([a-zA-Z] $|$ [0-9])*
\begin{itemize}
    \item . is any character (prof says followed by???), middle priority
    \item * is 0 or more repetitions, highest priority
    \item $|$ is the or operator, lowest priority
    \item () is the grouping operator
\end{itemize}
In ANTLR operands corresponds to FA edge labels, which are single char.
[a-z] is written as 'a'..'z' in ANTLR.
[0-9] is written as '0'..'9' in ANTLR.
\\ Let's define the regex for Integer:
\begin{enumerate}
    \item in english: $\epsilon$, + or - followed by a digit followed by 0 or more digits
    \item regex: (+ $|$ - $|$ $\epsilon$).[0-9].[0-9]*
    \item we can omit the .: (+ $|$ - $|$ $\epsilon$)[0-9][0-9]*
    \item we can omit the * using +: (+ $|$ - $|$ $\epsilon$)[0-9]+
\end{enumerate}
A DFA in practice is implemented as a 2D table T. 
One dimension is the state, the other is the input symbol. Every transition becomes T[i,a]=k. During execution I check the input the skip to the next state, very efficient.
\\ 
We could use flex to generate the lexer, the input is a set of regex and some C instruction. The output is C program that has a function yylex() which reads inputs and executes the C functions.

\subsection{Building a parser}
A parser checks the syntax of the program and builds a representation of the program.
\\ After reading the tokens provided by the lexer, the parser generates a parse tree and then the AST. The intermediate step between tokens and AST is rarely explicit, but for out purpose it will be.
\\ Let's parse this expression 4*(2+3).
\\ Parser input: NUM(4) TIMES LPAR NUM(2) PLUS NUM(3) RPAR
\\ Parse output AST
\\
\begin{tikzpicture}
      \Tree [.{$\ast$} 
    [.{4} ] 
    [.{$+$} 
      [.{2} ] 
      [.{3} ] 
    ] 
  ]
\end{tikzpicture}
\\ Parse tree
\\
\begin{tikzpicture}
    \Tree
    [.{EXPR} 
        [.{NUM(4)} ] 
        [.{TIMES} ] 
        [ .{EXPRESS} 
            [.{LPAR} ] 
            [.{EXPRESS}
                [.{NUM(2)} ] 
                [.{PLUS} ] 
                [.{NUM(3)} ] 
            ] 
            ] 
            [.{RPAR} ] 
        ] 
    ]
\end{tikzpicture}
Leaves are tokens.
\\ 
The AST is a simplified version of the parse tree, it's more compact and easier to process. You can think of the parse tree as concrete syntax.
\\ 
The parser therefore has 2 tasks: syntax checking and AST generation (from parse tree).
\\ We can a parser by hand but, same as the lexer, it's hard, errore prone and there are parser generators. We will write one manually just to see why it's no fun.
\\ The first problem is syntax checks: are parentheses balanced? We can't do this using the automa because (this is my guess) the language is non deterministic.
\\ Define the implementation: 
\begin{itemize}
    \item let TOKEN be enums: NUM, LPAR, RPAR, PLUS, MINUS,TIMES, DIV
    \item in[] is a global with the inputs
    \item next is a global with the index of next input
\end{itemize}
\begin{mycode}[title= Balanced parentheses parser]
    void Parse() {
        nextToken = in[next++];
        if (nextToken == NUM) return;
    }
    if (nextToken != LPAR) print("syntax error");
    Parse();
    if (in[next++] != RPAR) print("syntax error");
\end{mycode}
The output for (((1))) is:
\\
\begin{tikzpicture}
    \Tree [.{Parse()} 
    [.{(} ] 
    [.{Parse()} 
      [.{(} ] 
      [.{Parse()} 
          [.{(} ] 
          [.{Parse()} 1 ]
          [.{)} ] 
      ] 
      [.{)} ] 
    ] 
    [.{)} ] 
  ]
\end{tikzpicture}
Let's add subtraction: 
\begin{mycode}[title= Balanced parentheses parser]
    void Parse() {
        if ((nextToken = in[next++]) == NUM) {
            if (in[next] == MINUS) { next++; Parse(); }
        } else if (nextToken == LPAR) {
            Parse();
            if (in[next++] != RPAR) print("syntax error");
            if (in[next] == MINUS) { next++; Parse(); }
        } else print("syntax error");
    }
\end{mycode}
Obviously this is tedious, we will therefore use a parser generator.
To determine how our language will be generate we will 
use a CFG. Here's fore example a simple CFG for arithmetic expressions: E $\rightarrow$ n $|$ id $|$ ( E ) $|$ E + E $|$ E * E.
\\ CFG will guarantee that the there is no syntax error, how? One way is the naive solution, derive any possibile string and check if you program is among them. Actually used in real word (damn).

\section{Top-down parsing}
Let's parse a string. 
Recursive descent parsers try each production in turns, always left most, until there is a mismatch or a match.
\subsection{ Recursive descent parser}
Consider the following grammar:
\begin{itemize}
    \item E $\rightarrow$ T + E $|$ T
    \item T $\rightarrow$ (E) * T $|$ (E) $|$ int * T $|$ int
\end{itemize}
Let's parse the string $int_5$ * $int_2$, starting with E. 
\begin{enumerate}
    \item $E_0$ $\rightarrow$ $T_1 + E_1$
    \item $T_1$ $\rightarrow$ $(E_2)*T_2$, no token $int_5$
    \item $T_1$ $\rightarrow$ $(E_2)$, no token $int_5$
    \item $T_1$ $\rightarrow$ $int*T_2$, now we do all $T_1$ rules until we match. But after 
\end{enumerate}
\begin{tikzpicture}
    \Tree [.{$E_0$} 
    [.{$T_1$}  
    [.{$int_5$} ]
    [.{*} ]
    [.{$T_2$} [.{$int_2$} ]]
    ]
  ]
\end{tikzpicture}
\\ Define function that checks the token for match of the following: 
\begin{itemize}
    \item a given token terminal 
        \\ bool term(TOKEN tok) \{ return in[next++] == tok; \}
    \item a given production of S (the $n^{th}$) 
        \\ bool Sn() \{ ... \}
    \item any production of S:
        \\ bool S() \{ ... \}
\end{itemize}
Each function advance next. 
\\ Now let's get specific:
\begin{itemize}
    \item E $\rightarrow$ T + E
        \\ bool $E_1$() \{ return T() \&\& term(PLUS) \&\& E(); \}
    \item E $\rightarrow$ T
        \\ bool $E_2$() \{ return T(); \}
    \item any production from E: 
        \\ bool E() \{ 
            int save = next;
            return $E_1$() $||$ (next = save,$E_2$()); \}
\end{itemize}
Let's define non-terminal T functions: 
\begin{itemize}
    \item bool $T_1$() \{ return term(OPEN) \&\& E() \&\& term(CLOSE) \\ \&\& term(TIMES) \&\& T(); \}
    \item bool $T_2$() \{ return term(OPEN) \&\& E() \&\& term(CLOSE) \}
    \item bool $T_3$() \{ return term(INT) \&\& term(TIMES) \&\& T() \}
    \item bool $T_4$() \{ return term(INT) \}
    \item bool T() \{
            \\ int save = next;
            \\ return $T_1$() 
            \\ $||$ (next = save,$T_2$()) $||$ (next = save,$T_3$()) $||$ (next = save,$T_4$())
        \}
\end{itemize}
To start the parser next have to point to the first token and then you can invoke E(). If we have a special char \$ at the end of our token array, E() shall return true and the pointer to \$.
\\ This approach is easy to implement, but does not always work. Consider $S \rightarrow Sa$ this is a infinite loop, also called left-recursive grammar. We can fix the recursion. 
\\ Consider $S \rightarrow aS | b$, we can rewrite it as $S \rightarrow bS'$ and $S' \rightarrow aS' | \epsilon$.
\\ In general 
\[ S \rightarrow S\alpha_1 | ... | S\alpha_n | \beta_1 |...| \beta_m \]
should be rewritten as 
\[ S \rightarrow \beta_1S' | ... | \beta_mS' \]
\[ S' \rightarrow \alpha_1S' | ... | \alpha_nS' | \epsilon \]
ANTLR4 does this automatically.
\vspace{2mm}
\\ Another left recursive grammar is 
\[ S \rightarrow S\alpha | \delta \]
\[ S \rightarrow S\beta \]
can be eliminated using this algorithm: 
\\ List all non terminals symbols $A_1, A_2, ..., A_n$, for i:=1 to n: 
\begin{itemize}
    \item replace all $A_i \rightarrow A_j\beta$ such as j<i with $A_i \rightarrow \delta_1 \beta | \delta_2 \beta | ... | \delta_k \beta)$ where $A_j \rightarrow \delta | \delta_2 | ... | \delta_k$
    \item eliminate direct left recursion in $A_i$
    \item after i-th steps all productions are $A_i \rightarrow A_k\beta$ where k>1
\end{itemize}
This algorithm it's good enough for small languages but inefficient. 

\subsection{Predictive parsing}
It would be nice if the parse "knew" which production to expand next. 
\\ Let's try and replace: 
\begin{mycode}
return E() || (next = save,E2()) 
\end{mycode}
with 
\begin{mycode}
switch( something ) {
    case L1: return E1();
    case L2: return E2();
    otherwise: print ("syntax error");
}
\end{mycode}
"something" is the lookahead, the next token.
\\ This parser: 
\begin{itemize}
    \item uses the recursive descent but can predict which production to use
        \\ the prediction are made by looking the next otkesn
        \\ no backtracking
    \item accept LL(k) grammas 
        \\ L: left to right scan of inputs
        \\ L: leftmost derivation
        \\ k: k tokens of lookahead
    \item we will use LL(1) grammars
        \\ ANTLR can do LL(*) grammars
\end{itemize}
LL(1) means that for each non-terminal and token there is at most 1 production that leads to success. 
This method can be specified as a 2D table: 
\begin{itemize}
    \item one dimensions are non-terminals
    \item one dimensions are tokens
    \item each cell contains a production
\end{itemize}
For example the grammar: 
\\ E $\rightarrow$ T + E $|$ T
\\ T $\rightarrow$ (E) * T $|$ (E) $|$ int * T $|$ int
\\ is impossibile to predict, because for T 2 productions start with int and (. E is unclear how to predict. 
\\ In general a language must be \underline{left-factore}(ANTLR does this on it's own).
The new fixed grammar is:
\\ E $\rightarrow$ T + X 
\\ X $\rightarrow$ +E $|$ $\epsilon$
\\ T $\rightarrow$ (E) * Y $|$ int * Y
\\ Y $\rightarrow$ *T $|$ $\epsilon$

\subsubsection{LL(1) parser} 
For simplicity sake we will use a LL(1) \emph{table} and \emph{parse stack}.
\\ Here's our left-factore grammar 
\\ E $\rightarrow$ TX
\\ X $\rightarrow$ +E $| \epsilon$
\\ T $\rightarrow$ (E)Y $|$ int Y
\\ Y $\rightarrow$ *T $| \epsilon$
\newpage
The corresponding LL(1) parsing table: 
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|}
        \hline
        & int & * & + & ( & ) & \$ \\
        \hline
        T & int Y &  &  & (E)Y &  &  \\ 
        \hline
        E & T X &  &  & TX &  & \\ 
        \hline
        X &  &  & +E &  & $\epsilon$ & $\epsilon$ \\ 
        \hline
        Y &  & *T & $\epsilon$& & $\epsilon$ & $\epsilon$ \\
        \hline
	\end{tabular}
	\caption{Esempio di tabella}
\end{table}
The empty spots are errors. Like [E,*], we can't know that it's a string if it starts with *.
\\ We use a similar method to recursive descent. The difference is: we choose the production based on the next token. 
\\ There is a stack used to keep track on pending non terminals . 
We reject when there is an error and accept when we encounter end of input. 
\\ Here's the pseudo code of the algorithm: 
\begin{mycode}
    // We start the stack with <S,\$>
    repeat
        case stack of
        <X rest> : if T[X,*next] = Y1...Yn
            then stack := <Yn...Y1 rest>
            else error() 
        <t rest> : if t == *(next++)
            then stack := <Yn...Y1 rest>
            else error()
    until stack == <>
\end{mycode}
LL(1) languages are defined by a parsing table. 
No table entry can be multiply defined. 
We want to generate parsing tables from CFG.
\\ Consider the state $S\$ \xRightarrow[]{*} \beta A \gamma$, b is the next token and we want to match the string $\beta b \delta$.
\\ FIRST($\alpha$) is the set of terminals that are at the beginning of a string derived from $\alpha$, $\alpha$ is a generic string. If $\alpha \xRightarrow[]{*} \epsilon$ then $\epsilon \in$ First($\alpha$).
\\ FOLLOW(A), for non terminal A, is the set of terminals that can follow A in a derivation. There is such a derivation $S \xRightarrow[]{*} \alpha A a \beta$, a is in FOLLOW(A), \$ (the symbol that show the end of the stack) is also in FOLLOW(A).
\vspace{2mm} 
\\ Let's define formally first sets. 
First(X)=$\{ b | X \xRightarrow[]{*} b\beta \cup \{ \epsilon | X \xRightarrow[]{*} \epsilon \}$
\\ Therefore: 
\begin{itemize}
    \item First(b) = { b }
    \item For all productions $X \rightarrow A_1 ... A_n$ with n$\geq$0.
        \\ Add First($A_1$) - $\{ \epsilon \}$ to First(X). Stop if $\epsilon \notin$ First($A_1$).
        \\ Add First($A_2$) - $\{ \epsilon \}$ to First(X). Stop if $\epsilon \notin$ First($A_2$).
        \\ ...
        \\ Add First($A_n$) - $\{ \epsilon \}$ to First(X). Stop if $\epsilon \notin$ First($A_2$).
    \item repeat step 2 until there is no more First
\end{itemize}
Here's the definition with n$\geq$0: 
\\ First($X_1X_2...X_n$) = $\{ b | X_1X_2...X_n \xRightarrow[]{*} b\alpha\} \cup \{ \epsilon | X_1X_2...X_n \xRightarrow[]{*} \epsilon\}$. The steps are the same as above.
\vspace{2mm}
\\ Let's define formally follow sets: Follow(X) = $\{ b | S\$ \xRightarrow[]{*} \beta X b \gamma \}$.
\begin{enumerate}
    \item Add \$ to Follow(S), if S is the start symbols
    \item For all productions $Y \rightarrow \alpha X A_1...A_n$ with n$\geq$1 
    \\ Add First($A_1...A_n$) -\{ $\epsilon$ \} to Follow(X)
    \\ and for all productions $Y \rightarrow \beta X$ or $Y \rightarrow \alpha X \beta$ with $\epsilon \in First(\beta)$ 
    \\ Add Follow(Y) to Follow(X)
    \item repeat step 2 until there is no more Follow
\end{enumerate}
\vspace{3mm}
Here's an example of the algorithm. Using the grammar: 
\\ E $\rightarrow$ TX
\\ X $\rightarrow$ +E $| \epsilon$
\\ T $\rightarrow$ (E)Y $|$ int Y
\\ Y $\rightarrow$ *T $| \epsilon$
\\ First sets: 
\begin{itemize}
    \item Terminals: 
        \begin{itemize}
            \item First(+) = \{ + \} 
            \item First(*) = \{ * \}
            \item First(() = \{ ( \}
            \item First()) = \{ ) \} 
            \item First(int) = \{ int \}
        \end{itemize}
    \item Non-terminal: 
        \begin{itemize}
            \item First(T) = \{ (, int \} 
            \item First(X) = \{ +, $\epsilon$ \} 
            \item First(Y) = \{ *, $\epsilon$ \} 
            \item First(E) = \{ (, int \} 
        \end{itemize}
\end{itemize}
Follow sets:
\begin{itemize}
    \item Follow(E) = \{ \$, ) \} 
    \item Follow(X) = \{ \$, ) \} 
    \item Follow(T) = \{ +, \$, ) \} 
    \item Follow(Y) = \{ +, \$, ) \} 
\end{itemize}

\subsubsection{Constructing LL(1) parsing table}
Construct a parsing table T for CFG G. 
\\ For each production $A \rightarrow \alpha$ in G, do the following:
\begin{itemize}
    \item For each terminal b in First($\alpha$), T[A,a] = $\alpha$
    \item If $\epsilon \in$ First($\alpha$), for each b $in$ Follow(A) do, T[A,a] = $\alpha$
    \item If $\epsilon \in$ First($\alpha$) and \$ $\in$ Follow(A), T[A,\$] = $\alpha$
\end{itemize}
Notes on the algorithm:
\begin{itemize}
    \item If any entry is multiply defined, then G is not LL(1): 
        \\ if G is ambiguous
        \\ if G is left-recursive 
        \\ if G is not left-factored
        \\ and in other cases 
    \item Most programming grammas are NOT LL(1) 
    \item There are tools that build LL(1) tables that are fully declarative
\end{itemize}

\subsubsection{Ambiguity} 
A grammar is ambiguous if it has more than one parse tree for any string. This is bad and is common with arithmetic expressions and if then else.
\\ There are ways to deal with ambiguity, the most direct one is to rewrite the grammar, so that you can enforce symbol precedences.
\\ There is no way to automatically correct ambiguous grammar.

\section{Bottom-up parsing} 
Let's build a parser for this grammar: 
\\ E $\rightarrow$ E + T $|$ E - T $|$ T 
\\ T $\rightarrow$ T * int $|$ int
\\ This gramma is: 
\begin{itemize}
    \item non ambiguous 
    \item left recursive 
    \item not left factored
\end{itemize}
And our parser will not mind, because we will build it in revers. 
\\ E 
\\ E + T 
\\ T + T 
\\ T + T * int 
\\ T + int * int 
\\ int + int * int 
\\
\begin{forest}
  for tree={
    parent anchor=south,
    child anchor=north,
    align=center,
  },
  before typesetting nodes={
    for tree={
        if n children=0{
            tier=terminal
        }{}
    }
  }
  [E
    [E
        [T
            [int]
        ]
    ]
    [+
    ]
    [T
        [T
            [int]
        ]
        [*]
        [int]
    ]
    ]
  ]
\end{forest}
\\
Chaos bottom-up: 
\begin{enumerate}
    \item stare the input string s
    \item find in s a right-hand side or r of a production N $\rightarrow$ r
    \item reduce the found string r into non terminal N
    \item if all string reduced to start non terminal we'r done
    \item otherwise repeat from step 1
\end{enumerate}
This wont work, we will get stuck eventually: 
\\ E + E * E (we are stuck here)
\\ E + E * T 
\\ E + E * int 
\\ T + E * int
\\ int + E * int
\\ int + T * int
\\ int + int * int
\vspace{2mm} 
If you are lucky you might get by, but let's define ourt luck by using non-determinism.
\\ The new algorithm: 
\begin{enumerate}
    \item find all strings that can be reduced, let's say there are k of them
    \item create k copies of the input, k instances of the parser
    \item each instance reduces one of the k strings 
    \item if any instance reaches the start symbol we are done
\end{enumerate}
This will work, but it's not efficient. We could do exponential copies. We could reduce the instances using LR parser. Basically you will try any combination, but keep only 1 instance.
\\ Why won't this get me stuck? Because each tree follow a different derivation.
\\ Also keep in mind, the LR parser builds the right most derivation but in revers. Reads from left to right, and deriving the right most symbol: hence LR parser.
\\ Here's the action of an LR parser: 
\begin{itemize}
    \item The left part of the string will be on the stack 
        \\ the $\triangleright$ symbol is the top of the stack 
    \item We have 2 simple actions: 
        \\ reduce: replace a string on the stack 
    \item shift: move the $\triangleright$ symbol to the right
\end{itemize}
\newpage
This actions are chose non-deterministically.
\\ E $\triangleright$
\\ E + T $\triangleright$
\\ E + T * int $\triangleright$
\\ E + T * $\triangleright$ int 
\\ E + T $\triangleright$ * int 
\\ E + int $\triangleright$ * int
\\ E + $\triangleright$ int * int
\\ E $\triangleright$ + int * int 
\\ T $\triangleright$ + int * int 
\\ int $\triangleright$ + int * int 
\\ $\triangleright$ int + int * int
\\
\begin{forest}
  for tree={
    parent anchor=south,
    child anchor=north,
    align=center,
  },
  before typesetting nodes={
    for tree={
        if n children=0{
            tier=terminal
        }{}
    }
  }
  [E
    [E
        [T
            [int]
        ]
    ]
    [+
    ]
    [T
        [T
            [int]
        ]
        [*]
        [int]
    ]
    ]
  ]
\end{forest}
\\ The algorithm will be: 
\begin{itemize}
    \item find all reductions allowed on top of the stack 
    \item create k new instances of the parser
    \item each instance reduces one of the k strings, in the original instance do no reduction but a shift 
    \item stop when one instance is at the start
\end{itemize}
But we can do better, kill any instance that you know will get stuck ASAP.
\\ For example T + $\triangleright$ int * int, we know that this will get stuck, so we can kill it. This is because there is no way that T can be on top of the stack.
\\ How can we tell if a stack is doomed? We list all legal stacks and kill the illegal ones. The listed legal stacks are described as a DFA, if one configuration fails the DFA then it's doomed. In this DFA every state is accepting.

\subsection{Simple LR parser}
In order to produce a DFA we add a new initial variable 
E', and a new derivation E' $\rightarrow$ E.
\\ Our simple grammar becomes: 
\\ E' $\rightarrow$ E
\\ E $\rightarrow$ E + T $|$ E - T $|$ T
\\ T $\rightarrow$ T * int $|$ int
\\
\begin{tikzpicture}[shorten >=1pt,node distance=2cm,on grid,auto]
    \node[state, accepting] (q0) {q1};
    \node[state, accepting] (q1) [below=of q0]{q1};
    \node[state, accepting] (q2) [below=of q1]{q2};
    \node[state, accepting] (q3) [below=of q2]{q3};
    \node[state, accepting] (q4) [right=of q0]{q4};
    \node[state, accepting] (q5) [right=of q4]{q5};
    \node[state, accepting] (q6) [right=of q5]{q6};
    \node[state, accepting] (q7) [below=of q5]{q7};
    \node[state, accepting] (q8) [below=of q6]{q8};
    \node[state, accepting] (q9) [below=of q7]{q9};
    \path[->]
        (q0) edge node {T} (q1)
        (q1) edge node {*} (q2)
        (q2) edge node {int} (q3)
        (q0) edge node {E} (q4)
        (q4) edge node {+} (q5)
        (q5) edge node {T} (q6)
        (q0) edge node {int} (q9)
        (q4) edge node {-} (q7)
        (q6) edge node {*} (q2)
        (q8) edge node {*} (q2)
        (q7) edge node {int} (q9)
        (q7) edge node {T} (q8)
        ;
\end{tikzpicture}
\\ Each state is: 
\begin{itemize}
    \item q0:
        \\ E'$\rightarrow \circ$ E 
        \\ E $\rightarrow \circ$ E + T
        \\ E $\rightarrow \circ$ E - T
        \\ E $\rightarrow \circ$ T
        \\ T $\rightarrow \circ$ T * int 
        \\ T $\rightarrow \circ$ int
    \item q1:
        \\ E $\rightarrow$ T $\circ$ 
        \\ T $\rightarrow$ T $\circ$ * int
    \item q2:
        \\ T $\rightarrow$ T * $\circ$ int
    \item q3:
        \\ T $\rightarrow$ T * int $\circ$
    \item q4:
        \\ E' $\rightarrow$ E $\circ$ 
        \\ E $\rightarrow$ E $\circ$ + T
        \\ E $\rightarrow$ E $\circ$ - T
    \item q5:
        \\ E $\rightarrow$ E + $\circ$ T
        \\ T $\rightarrow$ $\circ$ T * int
        \\ T $\rightarrow$ $\circ$ int
    \item q6:
        \\ T $\rightarrow$ E + T $\circ$
        \\ T $\rightarrow$ T $\circ$ * int
    \item q7:
        \\ E $\rightarrow$ E - $\circ$ T
        \\ T $\rightarrow$ $\circ$ T * int
        \\ T $\rightarrow$ $\circ$ int
    \item q8:
        \\ E $\rightarrow$ E - T $\circ$
        \\ T $\rightarrow$ T $\circ$ * int
    \item q9:
        \\ T $\rightarrow$ int $\circ$
\end{itemize}
This is a SLR item: $X \rightarrow \alpha \circ \beta$, with $X \rightarrow \alpha \beta$ being a production. 
\\ $X \rightarrow \alpha \circ \beta$ describes the context: 
\begin{itemize}
    \item we are trying to find X 
    \item we have $\alpha$ on top of the stack
    \item we need to see the next a string derived from $\beta$
\end{itemize}
$\circ$ and $\triangleright$ are the same thing, but $\circ$ is used for SLR items and $\triangleright$ for LR items. A DFA state is a closet set of SLR(1) items, which means we perform Closure. The stating state is Closure(\{E' $\rightarrow \circ$ E \})
\\ The operation of extending the context with items is called closure operation: 
\\ Closure(Items) =
\\ \hspace*{4mm} repeat 
\\ \hspace*{8mm} for each X $\rightarrow \alpha \circ Y \beta$ in Items 
\\ \hspace*{12mm} for each production $Y \rightarrow \gamma$
\\ \hspace*{16mm} add $Y \rightarrow \circ \gamma$ to items
\\ until Items is unchanged
\vspace{2mm} 
\\ A State that contains at least an item, X $\rightarrow \alpha \circ y \beta$ has a transition label y to a state Transitions(State,y), y can be terminal or not.
\\ Transitions(State,y) = 
\\ \hspace*{4mm} Items $\leftarrow \emptyset$
\\ \hspace*{4mm} for each X $\rightarrow \alpha \circ y \beta$ in State
\\ \hspace*{8mm} add X $\rightarrow \alpha y \circ \beta$ to Items
\\ \hspace*{4mm} return Closure(Items)
\\ The SLR contains the correct stack configuration but also indicates when to shift/reduce. When the $\circ$ is at the end of the rule reduce, otherwise shift. If there are conflicts use the lookahead. If the look ahead finds a discrepancy, it will change the next transition. 
\\ The two rules are:
\begin{itemize}
    \item lookahead for shit: only on the expected terminals
    \item lookahead for reduce: only on terminals in the Follow set of the variabile on the l.h.s of the rules used to reduce
\end{itemize}
Consider our example grammar: 
\\ E' $\rightarrow$ E 
\\ E $\rightarrow$ E + T $|$ E - T $|$ T
\\ T $\rightarrow$ T * int $|$ int 
\\ First(E') = { int } (no need to calculate) 
\\ First(E) = { int } 
\\ First(T) = { int } 
\\ First(E') = { \$ } (no need to calculate) 
\\ First(E) = { \$, +, - } 
\\ First(T) = { \$, +, - } 
\\ Here's the SLR table representation 
\begin{table}[ht]
	\centering
	\begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        &int &+&-&*&\$&E&T\\ 
        \hline
        1 &shift,9& & && &goto,2&goto,5\\ 
        \hline
        2 &&shift,3&shift,6& &accept& & \\ 
        \hline
        3 &shift,9 & & & & & &goto,4\\ 
        \hline
        4 & &red,\linebreak E$\rightarrow$E+T&red,E$\rightarrow$E+T&shift,8&red,E$\rightarrow$E+T & & \\ 
        \hline
        5 &&red, E$\rightarrow$T& red, E$\rightarrow$T&shift,8&red, E $\rightarrow$ T& & \\ 
        \hline
        6 &shift,9& & & & & &goto,7\\ 
        \hline
        7&&red,E$\rightarrow$E-T&red,E$\rightarrow$E- T  &shift,8&red, E $\rightarrow$ E -T & & \\ 
        \hline
        8 &shift,10& & & & & & \\ 
        \hline
        9 & &red,T$\rightarrow$ int &red, T $\rightarrow$ int &red, T $\rightarrow$ int &red, T $\rightarrow$ int & & \\ 
        \hline
        10 &&red,T$\rightarrow$T*int &red,T$\rightarrow$T*int &red,T$\rightarrow$T*int &red,T$\rightarrow$T*int & & \\ 
        \hline
	\end{tabular}
	\caption{SLR(1) table}
\end{table}
This table describes the SLR(1) parsing algorithm. This works only if each cell have one action, meaning there is no conflict.
\\ In this case the grammar is called SLR(1) grammar.
\subsection{Bottom-up parsing algorithm}
This algorithm is stronger that LL(1) because the decision are made after seeing all the symbols not just one ahead.
\\ The parser represents the DFA as a 2D table. Each lines is a state, each column is a terminal or non-terminal. Columns are split into actions and goto.
\\ After each shift or reduce we rerun the DFA on the entire stack, this is a waste. A smarter approach is to save the state of the DFA after that element. 
\\ The stack is saved as such: $<sym_1, state_1> ...< sym_n, state_n>$.
\\ The algorithm is:
\\ Let I = $w_1w_2...w_n$\$ initial input
\\ Let j=i
\\ Let DFA state 0 be the starting state 
\\ Let stack = $< dummy, 0 >$
\\ \hspace{4mm} repeat
\\ \hspace{8mm} case $action[topState(stack), I[j])]$ of 
\\ \hspace{12mm} shift k: push $<$I[j],j$>$; j+=1
\\ \hspace{12mm} reduce X $\rightarrow \alpha$:
\\ \hspace{16mm} pop $|\alpha|$ pairs, 
\\ \hspace{16mm} push $< X, goto[topState(stack),X]>$
\\ \hspace{12mm} accept: halt normally
\\ \hspace{12mm} accept: halt and report error
\vspace{3mm}
\\ Bison it's a tool that takes a grammar and generates a parser. It's a LALR(1) parser generator, which is slightly more complex that a SLR(1).
\\ Bison takes in input grammar rules and C instructions and generates the table and a C program as output. 

\section{Semantic analysis}
\subsection{Symbol table}
We will learn how to build symbol tables and how to use them to find multiply-declared and undeclared variables.
\\ So far our compiler does: 
\begin{itemize}
    \item lexical analysis: detects illegal tokens
    \item parsing: detects ill formed parse tree
    \item semantic analysis: catches all remaining errors
\end{itemize}
Here's some typical semantic errors:
\begin{itemize}
    \item multiple declarations 
    \item undeclared variables 
    \item type mismatch
    \item wrong argument number/types
\end{itemize}
The semantic analysis is done in 2 steps (we visit the AST):
\begin{enumerate}
    \item Generation of Enriched AST (performed top-down)
        \\ For each scope of the programma: 
        \\ process declarations: add new entries to the symbol table and report any variable/methods method is multiply declared 
        \\ process statements: check that variables are declared and in "ID" nodes add a pointer to the appropriate symbol table entry
    \item type checking (performed bottom-up)
        \\ Process all of the statement in the program again, use the symbol-table entry information to type check
\end{enumerate}
The symbol table has to map names of anything declared in the program (variables, classes, fields, methods).
\\ The symbol table entry is a set of attributes associated with a name: 
\begin{itemize}
    \item kind of name (variable, class, field, method)
    \item type 
    \item nesting level
    \item memory location
\end{itemize}
The symbol table is designed based on the scoping of the language. Usually a variable can be re-declared once per scope.
\\ For example java uses these names: 
\begin{itemize}
    \item class 
    \item field of class 
    \item method of class
    \item local variable
\end{itemize}
Java and C++ support method overloading, method can have same name but have to be unique (by having different params or return). 
\\ Java and C++ use static scoping:
\begin{itemize}
    \item mapping are made at compile time 
    \item C++ uses the "most closely nested rule"
    \item in Java inner scopes cannot define variables defined in outer scopes
\end{itemize}
Each function has one or more scopes: one for params, one for body and any nested block inside.
\\ Not all languages use static scoping. For example python uses dynamic scoping, which means that the mapping are made at run time. This is not a great idea, a variable can have different declaration and different types.
\\ In Java it's possible to use a field or a method before the definition, but not a variable.
\\ From now on we will assume that our language: 
\begin{itemize}
    \item uses static scoping
    \item requires declaration before usage 
    \item does not allow multiply declarations in the same scope, no method overloading even with different kinds of names 
    \item does allow the same method to be defined multiply but once per scope
\end{itemize}
In addition our language will answer 2 questions: is this name already declared?, to which declaration does our name correspond?. Once we answer this, the table is no longer needed, names are forgotten.
\\ Here's the needed operations: 
\begin{itemize}
    \item look the name in the current scope, if not found insert a new name in the table
    \item look the name in the current and enclosing scope, to check and link undeclared names
    \item operations need when a new scope is entered
    \item operations need when a new scope is exited
\end{itemize}
The implementation can be a list of table or a table of lists. For simplicity we will assume each table only includes type and nesting level.

\subsubsection{List of hashtables}
The idea is a list of hashtables, one per scope.
\\ The process: 
\begin{itemize}
    \item On scope entry increment the current level number and add a new empty hashtable at the start of the list O(how hashtable size)
    \item To process a declaration of x, if it's in the first table the we error otherwise add x to the first hashtable with it's name, type and nesting level. (O(1)) lookup in hashtable is constant
    \item To process a use of x, search for x in the list, if found link it otherwise error. O(depth of nesting)
    \item On scope exit remove the first hashtable from the list. O(1) 
\end{itemize}

\subsubsection{Hashtable of lists} 
We have one big hashtable with all the names, and each name has a list of symbol-table entries. The first item is the closest scope. We also store the level number attribute, which tells us in which enclosing the declaration was made.
\\ The operations: 
\begin{itemize}
    \item On scope entry increment the current level number. O(1)
    \item To process a declaration of x, look up x in the symbol table, if x has the same nesting number error otherwise add a new item with appropriate current level number (O(1))
    \item To process a use of x, if there is an entry in the symbol-table link the use otherwise error (Undeclared variable) (O(1))
    \item On scope exit, scan all names at first level, if the level is the same as current then remove it from the list. If a name has empty list, then remove it. Finally decrement the current number O(number or names)
\end{itemize}

\subsection{Type checking}
Types are a set of values and operations on those values, like class is a type.
Type are needed because we can make operations with different types.
\\ There are 3 kinds of languages: 
\begin{itemize}
    \item statically typed: types are known at compile time (C)
    \item dynamically typed: types are known at run time (Scheme, lisp dialect...figures)
    \item untyped: no type checking (machine code)
\end{itemize}
The are lots of type wars, but the truth is that both are good. 
\\ Static typing avoids errors at compile time and overhead at run time. 
\\ Dynamic typing allows more flexibility and is easier to prototype.
\\ Type checking is the process of checking that the program obeys the type system. Type inference is when the compiler automatically deduces the type.
\\ We have so far 2 formal notations: regex for lexer and CFG for parser. The way to enforce type checking is having logical rules of inference. Inference rules have this form: \emph{If Hypothesis is true, then Conclusion is true} therefore \emph{If $E_1$ and $E_2$ have certain types, then $E_3$ has a certain type}.
\\ The notation is simple to read, here's a few building blocks: 
\begin{itemize}
    \item $\wedge$ is "and" 
    \item $\Rightarrow$ is "if then"
    \item x:T is "x has type T"
\end{itemize}
Here's an example: 
\\ If $e_1$ has type Int and $e_2$ has type Int then $e_1 + e_2$ has type Int
\vspace{2mm} 
\\($e_1$ has type Int $\wedge$ $e_2$ has type Int) $\Rightarrow$ $e_1 + e_2$ has type Int
\vspace{2mm} 
\\($e_1$: Int $\wedge$ $e_2$: Int) $\Rightarrow$ $e_1 + e_2$: Int

\begin{mathpar}
  \inferrule
    {\vdash e_1:Int \\ \vdash e_2:Int}
    {\vdash e_1 + e_2: Int}
\end{mathpar}

$\vdash$ means "we can prove that..."
\\ An inference system for proving x$<$y: 
\begin{mathpar}
  \inferrule
    {}
    {\vdash x < (x+1)}
\end{mathpar}
[A] means that A is an axiom, and in the case above it means that any number is smaller than it's successor.
\begin{mathpar}
  \inferrule
    {\vdash x < y \\ \vdash y < z}
    {\vdash x < z}
\end{mathpar}
[T] means that the rule is transitive. Therefore $<$ is transitive.
\\ Let's prove that 6$<$10.
% write infer rule for 6<10 
\begin{mathpar}
    \inferrule
    { \inferrule 
        { }
        {\vdash 6 < 7} [A]
        \and 
        { \inferrule 
            { \inferrule 
                { \inferrule 
                    { } 
                    {\vdash 7 < 8} [A] 
                    \and 
                    { }
                    {\vdash 8 < 9} [A]
                } 
                {\vdash 7 < 9} [T]
                \and 
                { } 
                {\vdash 9 < 10} [A]
            } 
            {\vdash 7 < 10} [T]
        }
    }
    {\vdash 6 < 10} [T]
\end{mathpar}
In our case 
\begin{mathpar}
    \inferrule 
    { \inferrule
        { } 
        { \vdash 1:Int }
        \and 
        { \inferrule 
            { }
            { \vdash 2:Int }
        }
    }
    { \vdash 1 + 2:Int }
\end{mathpar}
A type system is sound if we can infer $\vdash$e:T.
\\ Here's rules for constants: 
\begin{mathpar}
    \inferrule
    { }
    { \vdash false: Bool } [Bool]
\end{mathpar}
\begin{mathpar}
    \inferrule
    { }
    { \vdash s: String } [String] \text{(s is constant)}
\end{mathpar}
Object creation: 
\begin{mathpar}
    \inferrule
    { }
    { \vdash new \; C(): C} [New] \text{(C is a class)}
\end{mathpar}
more rules 
\begin{mathpar}
    {
        \inferrule
        { \vdash e: Bool}
        { \vdash not \; e: Bool} \; [Not] 
    }
    \\
    { 
        \inferrule
        { \vdash e_1: Bool \\ \vdash e_2: T}
        { \vdash while \; e_1 \; loop \; e_2 \; pool: Stmnt} \; [Loop]
    }
\end{mathpar}
Let's type: while not false loop 1 + 2 * 3 pool
\\
\begin{forest}
  for tree={
    parent anchor=south,
    child anchor=north,
    align=center,
  },
  [ while loop: Stmnt
    [ not: Bool
        [ false: Bool ]
    ]
    [ + : Int
        [ 1: Int ]
        [ * : Int
            [ 2: Int ]
            [ 3: Int ]
        ]
    ]
  ]
\end{forest}
\begin{mathpar}
    \inferrule 
    { \inferrule 
        { \vdash false: Bool } 
        { \vdash not \; false: Bool } 
        \and 
        { \inferrule 
            { \inferrule 
                { } 
                { \vdash 1: Int } 
                \and 
                { \inferrule 
                    {
                        { \vdash 2: Int }
                        \and
                        { \vdash 3: Int }
                    }
                    { \vdash 2*3: Int }
                }
            } 
            { \vdash 1+2*3: Int }
        }
    }
    { \vdash while \; not \; false \; loop \; 1 + 2 * 3 \; pool: Stmnt } 
\end{mathpar}
\begin{itemize}
    \item The root of the tree is the whole expression 
    \item Each node is an instance of typing rule 
    \item Leaves are the rules with no hypotheses
\end{itemize}
But how can we know the type of a variable? 

\begin{mathpar}
    \inferrule 
    { } 
    { \vdash x: ? } [Var] \; \text{(x is an identifier)}
\end{mathpar}
We don't know enough, it's need to have knowledge of the scope. So let's put more information in the scope.
\\ Let O be a function from Identifiers to Types: 
\\ The sentence O $\vdash$ e:T reads: 
\\ Under the assumption that the variables in the current scope have the types given by O, it is provable that expression e has type T (this are info store in the symbol table btw).
\\ A type environment O gives types for \textbf{free} variables. A free variable is a variable out of current scope.
\\ Here's the previous rules but modified: 
\begin{mathpar}
    {
        \inferrule
        { }
        {O \vdash i: Int} [Bool]
    }
    \\
    {
        \inferrule
        { O \vdash e_1: Int \\ O \vdash e_2: Int }
        {O \vdash e_1 + e_2: Int} \; [Add]
    }
\end{mathpar}
Now we can write new rules: 
\begin{mathpar}
    \inferrule 
    { }
    { O \vdash x: T } \; \; [Id] \; \; \text{(if O(x)=T)}
\end{mathpar}
The types of a function are usually written as: 
\begin{itemize}
    \item $T_1,...,T_n \rightarrow T$
    \item with $T_1,...,T_n \rightarrow T$ being the params
    \item and T the output return
\end{itemize}
\begin{mathpar}
    \inferrule 
    { O \vdash f:(T_1,...,T_n \rightarrow T) \\ O \vdash e_1:T_1 \; ... \; O \vdash e_n:T_n }
    { O \vdash f(e_1,...,e_n):T } \; \; [FuncCall]
\end{mathpar}
The let statement declares a variable x with given type $T_0$ that is then defined throughout $e_1$: 
\begin{itemize}
    \item $let \; x:T_0 \; in \; e_1$ without initialization
    \item $let x: T_0 \leftarrow e_0 in e_1$ with initialization
\end{itemize}
\begin{mathpar}
    \inferrule 
    { O[T_0/x] \vdash e_1:T_1 }
    { O \vdash let \; x:T_0 \; in \; e_1:T_1 } \; \; [Let-No-Init]
\end{mathpar}
For example: 
\\ let x: $T_0$ in ( (let y: $T_1$ in $E_{x,y}$) + (let x: $T_2$ in $F_{x,y}$)
\\ $E_{x,y}$ and $F_{x,y}$ are expressions that use x and y
\\ The type of x in $E_{x,y}$ is $T_0$ and the type of x in $F_{x,y}$ is $T_2$.
\\
Notes: 
\begin{itemize}
    \item The type environment gives types to free identifier in current scope 
    \item The type environment is passed down the from root to leaves
    \item Types are computed from leaves to root
\end{itemize}
The downward phase of compilers enrich the AST linking identifies with the their symbol table entry.
\vspace{3mm} 
\\ Consider let with initialization: 
\begin{mathpar}
    \inferrule 
    { O \vdash e_0: T_0 \\ O[T_0/x] \vdash e_1: T_1} 
    { O \vdash let  \; x:T_0 \leftarrow e_0  \; in \; e_1:T_1} \;\; [Let-Init]
\end{mathpar}
This rule is too restrictive. Because it does not allow inheritance. 
\\ class C inherits P { ... }
\\ let x: P $\leftarrow$ new C() in ...
\vspace{4mm}
\\ Let's add a relation X $\leq$ Y on classes that means: 
\begin{itemize}
    \item X can be used when Y is acceptable or equivalent 
    \item X conforms with Y 
\end{itemize}
Also know as X is a subclass of Y. Let's define de relations $\leq$: 
\begin{itemize}
    \item X $\leq$ Y
    \item X $\leq$ Y if X inherits from Y
    \item X $\leq$ Z if X $\leq$ Y and Y $\leq$ Z
\end{itemize}
Let's re-write our Let rule:
\begin{mathpar}
    \inferrule 
    { O \vdash e_0: T_0 \\ T \leq T_0 \\ O[T_0/x] \vdash e_1: T_1} 
    { O \vdash let  \; x:T_0 \leftarrow e_0  \; in \; e_1:T_1} \;\; [Let-Init]
\end{mathpar}
It's still sound but more flexible. There is tension between flexible and sounds rules.
\\ A static type system enables the compiler to detect many common errors but at the cost of disallowing correct programs. Some argue that the solution is more expressive type system, but this leads to more complex language. Others argue that you should use dynamic type instead.
\\ The static type of an object variable is the class C used to declare the variable, a compile-time notion.
\\ The dynamic type of an object variable is the class C that is used to create its object \textbf{value} ("new C"), a run-time notion.
\\ In early type systems those two corresponded (dynamicType(E) = staticType(E)), but in modern languages they don't. You could for instance change class of an object a with class A, if class B inherits from A. Hence dynamicType(E) $\leq$ staticType(E). Soundness Theorem 
\\ $\forall$E. dynamicType(E) $\leq$ staticType(E)
\\ This works if the subclass has all public methods and fields of the parent class.
\vspace{4mm} 
\\ We also need a rule if x is already declared: 
\begin{mathpar}
    \inferrule 
    { O \vdash e_0: T \\ T \leq T_0 \\ O \vdash e_1:T_1} 
    { O \vdash x \;\; e_0 \;; \;\; e_1:T_1} \;\; \;\;\text{[Assign] (if O(x) = T\textsubscript{0}}
\end{mathpar}
Let's add subtyping to functions, our input parameters need to account for subtyping $T_{0,1},...,T_{0,n} \rightarrow T$
\begin{mathpar}
    \inferrule 
    { O \vdash f:(T_{0,1},...,T_{0,n} \rightarrow T) \\ O \vdash e_1:T_1 \; ... \; O \vdash e_n:T_n \\ \forall i T_i \leq T_{0,i}}
    { O \vdash f(e_1,...,e_n):T } \; \; [Fun]
\end{mathpar}
You can subtype arrays but only the content of the cells.
\vspace{5mm} 
\\ Classes usually can override methods and fields. This is the case in our language FOOL. 
\\ Let's start from fields. 
\begin{itemize}
    \item Class A \{ ..., T:f, ... \}
    \item Class B inherits from  \{ ..., T':f, ... \}
\end{itemize}
Fields can be seen as arrays cells, they can be modified but can't change type. Fields subtyping is not sound, that's why in java we cannot override fields. But if fields are immutable then subtyping is admitted, if T'$\leq$T for every fields then B $\leq$ A. These are calle \emph{covariant fields}.
\vspace{4mm} 
\\ Consider methods overriding by changing the return type and the params: 
\begin{itemize}
    \item Class A \{ ..., T : m(T$_1$ p$_1$,...,T$_n$ p$_n$)\{e\} ... \}
    \item Class B inherits from  \{ ..., T' m(T'$_1$ p$_1$,...,T'$_n$ p$_n$) \{e' \} ... \} 
\end{itemize}
Consider "let x:T $\leftarrow$ y.m($e_1,...,e_n$) in e" assuming "y" with static type A and dynamic type B. 
\begin{itemize}
    \item the B return type T' must be usavble in place of the A return type T, T'$\leq$T.
    \item the params types must be usable in place of B params types, T$_i \leq$ T'$_i$
\end{itemize}
The general subtyping rule for functions is: 
\begin{mathpar}
    \inferrule 
    { T_1 \leq T'_1 ... T_n \leq T'_n \text{ and } T' \leq T }
    { (T'_1,...,T'_n \rightarrow T') \leq (T_1,...,T_n \rightarrow T) }
\end{mathpar}
"covariant" output return type
\\ "contravariant" input params types. 
\vspace{5mm} 
\\ You have to make a tradeoff, the less sound the language then more bad programs will be accepted. The more sound the language, the less good program will be accepted.

\section{Code generation}
We covered the front-end phases: lexical analysis, parsing, semantic analysis. Now we will cover the back-end phases: code generation and optimization.

\subsection{Memory management} 
The initial execution of the program is in the hands of the computer. The OS allocates memory for the program, the code in loaded and the OS jumps to the entry point (the main).
\\ The memory get's split into code and other space (not necessarily contiguous memory). The other space is usually data. 
\\ The compiler is responsible for generating the code and organizing the use of the data area. The goals for memory management is to be fast and correct, which is hard.
\\ Let's make some assumptions about executions: 
\begin{enumerate}
    \item Execution is sequential
    \item When a procedure (functions) is called, we save the pointer to the current position, execute and return to current position
\end{enumerate}
An invocation of procedure P is an activation of P. The \emph{lifetime} of P is: all the steps to execute P, including any procedures P calls.
\\ The \emph{lifetime} of a variable x is the portion of the execution in which x is defined. The difference between variable scope and lifetime is: lifetime dynamic run-time concept while scope is a static concept.
\\ Our next assumption (2) is that when P calls Q, then Q returns before P does (like in recursions). Lifetimes of procedure activations are nested and can be depicted as trees.
\begin{mycode}
    class Main {
        int g() { return 1; } 
        int f() { return g(); }
        void main() { g(); f(); }
    }
\end{mycode}
\begin{forest}
  for tree={
    parent anchor=south,
    child anchor=north,
    align=center,
  },
  [ main
    [ g ]
    [ f
        [ g ]
    ]
  ]
\end{forest}
\\ Note: 
\begin{itemize}
    \item The activation tree depends on run-time behavior
    \item The activation tree may be different for every program input
    \item Currently active procedures are tracked in a stack
\end{itemize}

Memory \\
\begin{tikzpicture}[scale=0.8]
  \draw[fill=blue!20, draw=blue] (2,-1.5) rectangle node[midway,align=center] {Code} ++(4,-1);
  \draw[fill=green!20, draw=green] (2,-2.5) rectangle node[midway,align=center] {Stack} ++(4,-1);
  \draw[fill=red!20, draw=red] (2,-3.5) rectangle node[midway,align=center] { } ++(4,-1);
\end{tikzpicture}

The information needed to manage one procedure activation is calle \textbf{activation record} or \textbf{frame}. If procedure f calls g, the g's activation record contains a mix of info about f and g.
\\ When f calls g, f is "suspended" until g completes. f will be resumed by g, g knows how thanks to it's AR.
\\ g's AR could contain:
\begin{itemize}
    \item the return address of f
    \item g params
    \item space for g's local variables
    \item other temporary values
\end{itemize}
g's AR should have: 
\begin{itemize}
    \item pointer to the previous AR, called control link and points to the caller of g
    \item machine status prior to calling g, contents of the registers and program counter (pointer to current position in stack).
\end{itemize}
The compiler therefore must determine at compile time the \textbf{layout} of the activation record and generate code that access correctly the locations in the activation record: AR layout and code generator must be designed together.
\\ You could have a simple stack that has a fixed offset which works, but if organized it could help speed and simplicity.
\\ All reference to global variables have a fixed address, they can be statically or dynamically allocated.
\\
    \begin{tikzpicture}[scale=0.8]
  \draw[fill=blue!20, draw=blue] (2,-1.5) rectangle node[midway,align=center] {Code} ++(4,-1);
  \draw[fill=yellow!20, draw=yellow] (2,-2.5) rectangle node[midway,align=center] {Static data} ++(4,-1);
  \draw[fill=green!20, draw=green] (2,-3.5) rectangle node[midway,align=center] {Stack} ++(4,-1);
  \draw[fill=red!20, draw=red] (2,-4.5) rectangle node[midway,align=center] { } ++(4,-1);
\end{tikzpicture}
\\ Reference to a variable declared in an outer scope should point to a variable stored in another record. Any block has it's own activation record.
In particular it should point to the most recent AR within the immediately enclosing scope. 
\\ The access link is used to find outer scope variables and always point to the enclosing syntax block, like say a functions.
\\ The value of the access link is defined as follows: 
\begin{itemize}
    \item when we enter a new scope or create a function: access link = current AR (we save the current access link address)
    \item a function calls herself or another function: access link = caller's access link
    \item call of a function outside the current scope: access link = follow the chain of access link until you find the correct one
\end{itemize}
Any data that outlives the procedure can't be saved in the AR, such data must be saved in the \emph{heap}.
\\ Unlike di AR data the heap data should be remove when it becomes garbage. The garbage collection is done by the system when needed, it is execute at the same time as the program.
\\ In some languages like C the deallocation is under the responsibility of the programmer which can lead to big problems when pointers share the same memory.
\vspace{4mm} 
\\ Mark and sweep algorithm tags all resources with 0, then follows all links and set those tags to 1.
Finally removes any tag that is 0.
\vspace{4mm} 
\\ Reference counting algorithm counts the number of references to a resource, when it reaches 0 it removes the resource. 
When the pointer is set to a resource it increments the counter, when the pointer is set to null it decrements the counter.
\\ Datum = data element, which is basically a resource.
\vspace{4mm} 
\\ Notes:
\begin{itemize}
    \item the code portion contains object code, usually fixed size and read only
    \item the static area contains data with fixed addresses, fixed size and could be readable and writable
    \item the stack contains activation records, grows and shrinks dynamically. A single procedure is usually fixed size and contains locals
    \item heap contains all the other data and it can grow
    \item heap and stack could grow into each other! One way to avoid it's to have them at opposite memory locations.
\end{itemize}

\begin{tikzpicture}[scale=0.8]
  \draw[fill=blue!20, draw=blue] (2,-1.5) rectangle node[midway,align=center] {Code} ++(4,-1);
  \draw[fill=yellow!20, draw=yellow] (2,-2.5) rectangle node[midway,align=center] {Static data} ++(4,-1);
  \draw[fill=green!20, draw=green] (2,-3.5) rectangle node[midway,align=center] {Stack} ++(4,-1);
  \draw[fill=gray!20, draw=gray] (2,-4.5) rectangle node[midway,align=center] { Unallocated } ++(4,-1);
  \draw[fill=purple!20, draw=purple] (2,-5.5) rectangle node[midway,align=center] { Heap } ++(4,-1);
\end{tikzpicture}

\subsection{Code generation for stack machine} 
It's a machine that computes using a stack. 
Both the operations and the results are pushed on the stack.
\\ Say you want to add to numbers: push 5, push 7, add.
The add operation will pop 5 and 7, add them and push the result.
\\ The big advantage of the stack machine is it's simplicity:
\begin{itemize}
    \item input and output in the same place
    \item uniform compilation which means simple compiler
    \item the stack is invariant, results stack on top22
    \item items are always on top of the stack: location is implicit
    \item no need to specify operands and results 
    \item compact programs and small instructions 
    \item java bytecode is a stack machine
\end{itemize}
Any operation need the stack will perform 2 reads and 1 write (2 pops and 1 push). We could optimize this by keeping the top of the stack in a register (accumulator) therefore removing one read. We will not do it, for simplicity sake.

\subsubsection{From stack machine to assembly language} 
We will consider a stack machine without an accumulator.We want to run this code on a processor, in our case it will be MIPS (Microprocessor without Interlocked Pipeline Stage). There are several emulators of this processor.
To be more precise we implement the stack machine using MIPS instructions and registers.
\\ MIPS architecture: 
\begin{itemize}
    \item Prototype RISC (Reduced Instruction Set Computer) architecture
    \item arithmetic operations use register for operands and results 
    \item use load and store instructions to access memory
    \item 32 general use registers, each 32 bits. We will use \$sp and \$t$_0$,\$t$_1$,\$t$_2$ (temporary registers)
\end{itemize}
Therefore the stack will be kept in memory and grow towards lower addresses. The \$sp register keeps track of the top of the stack. Let's define some macros: 
\begin{itemize}
    \item push \$t becomes addiu2 \$sp, \$sp, -4; sw \$t, 0(\$sp)
    \item pop \$t becomes addiu \$sp, \$sp, 4
    \item \$t $\rightarrow$ pop becomes lw \$t, 0(\$sp); addiut \$sp, \$sp, 4
    \item pop*n becomes addiu \$sp, \$sp, z where z = 4*n
\end{itemize}
Let's define a small language with integers, integer operation and global functions with at least one parameter.
\\ \hspace*{5mm} P $\rightarrow$ D ; P $|$ S
\\ \hspace*{5mm} D $\rightarrow$ fun id(ARGS) = E
\\ \hspace*{5mm} ARGS $\rightarrow$ id, ARGS $|$ id
\\ \hspace*{5mm} E $\rightarrow$ int $|$ id $|$ if E$_1$ = E$_2$ then E$_3$ else E$_4$ $|$ E$_1$ + E$_2$ $|$ E$_1$ - E$_2$ $|$ id( E$_1$, ..., E$_n$)
\\ \vspace{3mm} The first function definition f is the main routine.
\\ Here's the program for computing Fibonacci: 
\\ \hspace{2mm} fun fib(x) = if x=1 then 0 else:
\\ \hspace{2mm} if x=2 then 1 else 
\\ \hspace{2mm} fib(x-1) + fib(x-2)
\vspace{3mm}
\\ For each expression $e$ we will generate MIPS code that: 
\begin{itemize}
    \item computes the result of $e$ and pushes it to the stack
    \item preserves the content of the stack that came before $e$, we simply add on the top the result of $e$
\end{itemize}
cgen(e) is generation function which generates code for $e$. Eg: Say you have a constant $i$, the code cgen(i) = li \$t$_0$ i ; push \$ t$_0$.
\\ Let's see cgen(e$_1$, e$_2$): 
\begin{enumerate}
    \item cgen(e$_1$)
    \item cgen(e$_2$)
    \item \$t$_2 \leftarrow$ pop
    \item \$t$_1 \leftarrow$ pop
    \item add \$t$_0$, \$t$_1$, \$t$_2$
    \item push \$t$_0$
\end{enumerate}
What if the put the result in t$_1$ to optimize operations? We would have wrong code! You can't preserve the value of a register between operations!
\\ Code generation notes: 
\begin{itemize}
    \item stack machine code generation is recursive 
    \item code generations is performed bottom-up by a recursive traversal of the AST.
\end{itemize}
Let's add sub to our instructions: sub reg$_0$ reg$_1$ reg$_2$ or reg$_0 \leftarrow$ reg$_1$ reg$_2$.
\\ cgen(e$_1$ - e$_2$):
\begin{itemize}
    \item cgen(e$_1$) 
    \item cgen(e$_2$)
    \item \$t$_2 \leftarrow$ pop 
    \item \$t$_1 \leftarrow$ pop 
    \item sub \$t$_0$, \$t$_1$, \$t$_2$ 
    \item push \$t$_0$
\end{itemize}
We also need to add flow control instructions: beq reg$_1$ reg$_2$ label or if reg$_1$ = reg$_2$ then goto label.
\\ Another new instruction is b label, unconditional jump to label.
\\ cgen(if e$_1$ = e$_2$ then e$_3$ else e$_4$): 
\\ \hspace*{4mm} cgen(e$_1$)
\\ \hspace*{4mm} cgen(e$_2$)
\\ \hspace*{4mm} \$t$_2 \leftarrow$ pop 
\\ \hspace*{4mm} \$t$_1 \leftarrow$ pop 
\\ \hspace*{4mm} beq \$t$_1$, \$t$_2$, true\_branch
\\ \hspace*{4mm} cgen(e$_4$) 
\\ \hspace*{4mm} b end\_if 
\\ true\_branch: 
\\ \hspace*{4mm} cgen(e$_3$) 
\\ end\_if:
\\ You are supposed to also generate your labels with cgen().
\\ Code for functions and functions definition depend on the activation record layout.
In our case a simple AR will suffice: 
\begin{itemize}
    \item the result of the function is on top of the stack, no need to reserve space for the result in the AR
    \item the AR holds parameters values, these are the only variables in our current language
\end{itemize}
We still need to store the return address and the control link. The access link is still not needed, we don't have nested declarations, we have local params and global functions.
\\ It's handy to have a pointer to the current position (reference pointer) in the AR.
This pointer is saved in \$fp (frame pointer).
The reference position is part of AR layout design. \$fp points to the position of the first argument and it's used by the code generator to locate elements (params, based on offset).
\\ We need to store the caller \$fp (control link) because it has to be restored after the call.
\\ Now let's add a new instructions: jal label (jump and link). Jump to label and save the return address in \$ra register.
\\ Code generation for a function call: 
\\ cgen(f(e$_1$, ..., e$_n$)):
\\ \hspace*{4mm} push \$fp
\\ \hspace*{4mm} cgen(e$_n$)
\\ \hspace*{4mm} ...
\\ \hspace*{4mm} cgen(e$_1$)
\\ \hspace*{4mm} jal f\_entry
\\ We save the frame pointer, push each value in reverse order, save the return address in \$ra and so far our memory used is 4*n+4 bytes (4 bytes for each param and 4 byte for the \$fp).
\\ Let's add another istruction: jr reg (jump register). Jump to the address contained in reg.
\\ cgen(fun f(x$_1$, ..., x$_n$) = e):
\\ \hspace*{4mm} f\_entry:
\\ \hspace*{4mm} move \$fp, \$sp
\\ \hspace*{4mm} push \$ra
\\ \hspace*{4mm} cgen(e)
\\ \hspace*{4mm} \$t$_0 \leftarrow$ pop 
\\ \hspace*{4mm} \$ra $\leftarrow$ pop 
\\ \hspace*{4mm} pop*n 
\\ \hspace*{4mm} $fp \leftarrow$ pop 
\\ \hspace*{4mm} push \$t$_0$ 
\\ \hspace*{4mm} jr \$ra
\\ Save the current stack in \$fp into \$sp, save the return address in the temporary field \$t$_0$, we do all the operations save the return and jump to the \$ra. 
\\ Activation record are \textbf{not} saved adjacently in the stack, because the stack grows during execution.
\\ Be mindful, the \$sp always changes! We can't use to perform operations with our params, however this is not true for the \$fp. Also remember that the \$fp keeps the location of the first param.
\\ Therefore in fun (f(x$_1$, x$_2$) = e) x$_1$ is at \$fp and x$_2$ is at \$fp + 4. 
\\ Thus based on the offset of the params x$_i$: cgen(x$_i$) = lw \$t$_0$ z(\$fp); push \$t$_0$. The offset is z=4*(i-1) and should be saved inside the symbol table. 

\subsubsection{Code generation for OOP languages}
We have 2 issues: 
\begin{itemize}
    \item How to represent objects in memory.
    \item How to dynamically dispatch methods.
\end{itemize}
Consider the following code: 
\begin{mycode}
    class A { 
        int a = 0;
        int d = 1;
        int f() { return a = a + d; }
    }

    class B extends A {
        int b = 2;
        int f() { return a; }
        int g() { return a = a - b; }
    }

    class C extends B {
        int c = 3;
        int h() { return a = a * c; }
    }
\end{mycode}



\newpage
\import{}{esercizi}
\end{document}
